---
title: "Homework 1"
author: "Umberto Junior Mele 1388371"
date: "13 marzo 2017"
output: pdf_document
---


a)

$Y_i$ is a random variable with distribution $Bernoulli$ and $i.i.d.$ so we can write down the joint distribution, like this:

$$Pr(Y_{1}=y_1,......, Y_{100}= y_{100} |\theta) = \prod_{i=1}^{100} \theta^{y_i}(1 -\theta)^{1-y_i}= L(\theta)$$
where $L(\theta)$ is the Likelihood function.

While the probability distribution of $Pr(\sum_{i=1}^{100} Y_i = y |\theta)$
is defined by the random variable $Z=Y_1 + .... + Y_{100}$, that is distribuited like a Binomial distribution, so:
$$Pr(\sum_{i=1}^{100} Y_i = y |\theta) = Pr(Z=y) = \binom{100}{y} \theta^y (1 - \theta)^{100-y} $$



2)

```{r}
theta <- seq(0,1,0.1)
prob_Z.equal.57 <- dbinom(57 ,100, theta)
plot(theta, prob_Z.equal.57, xlab = 'theta' , ylab = 'prob')
```



3)
Remembering that the posterior distribution is:
$$\pi(\theta | x) = \frac{L(\theta) \cdot \pi(\theta)}{m(x)}$$

```{r}
prior <- function(t)  1/11 *(t < 1 & t >0)
L <-  function(t) dbinom(57,100,t)
m <- function(all.theta){
  su <- 0
  for(t in all.theta){
    su <- su + prior(t)*L(t)
  }
  return(su)
}

posterior <- function(t) prior(t)*L(t)/m(theta)
posterior <- Vectorize(posterior)

plot(theta ,posterior(theta), ylab = 'posterior')
curve(posterior(x), add = T, col='blue')

```

```{r}
prior <- function(t) dunif(t)*(t>0 & t<1 )
L <-  function(t) dbinom(57,100,t)
joint <- function(t) L(t)*prior(t)
m <- integrate(joint,0,1)

posterior <- function(t) prior(t)*L(t)/m$value
posterior <- Vectorize(posterior)

curve(posterior, col = 'violet')


```


e)
We know that the posterior distribution is a $Beta(1+57, 1 +43)$ because if we use the Conjugate Analysis for this problem, so we use a $Beta$ distro as Prior:

$$\pi(\theta | \bar{Z}) = \binom{100}{57} \theta^{57} (1 - \theta)^{100-57} \cdot \frac{\theta^{\alpha - 1}(1 - \theta)^{\beta - 1}}{\beta(\alpha,\beta)} \propto \theta^{57 + \alpha-1} (1 - \theta )^{43 + \beta -1}$$

so the posterior distribution is a $Beta$ distribution with parameters $\dot\alpha = 57 + \alpha$ and $\dot\beta = 43 + \beta$, so the prior distribution is a $Beta(1,1)$ that is the same distribution of a $Unif(0,1)$.


```{r}
curve(dbeta(x,1,1))
```
```{r}
prior <- function(t) dbeta(t,1,1)*(t>0 & t<1 )
L <-  function(t) dbinom(57,100,t)
joint <- function(t) L(t)*prior(t)
m <- integrate(joint,0,1)

posterior_3 <- function(t) prior(t)*L(t)/m$value
posterior_3 <- Vectorize(posterior_3)

curve(posterior_3, col = 'green')

```





2)
a)

The prior predictive distribution is defined by:
$$m(\cdot) = \int f(\cdot | \theta) \pi(\theta) d\theta = \int \frac{\sqrt{\lambda}}{\sqrt{2\pi}}e^{- \frac{\lambda(x - \theta)^2}{2}} \cdot \frac{\sqrt{\nu}}{\sqrt{2\pi}}e^{- \frac{\nu(\theta - \mu)^2}{2}} d\theta $$
$$ \propto  \int e^{-\frac{\lambda(x^2- 2 \theta x +\theta^2)+ \nu(\theta^2 - 2\theta \mu + \mu^2)}{2}}d\theta $$
but we can use also a trick ...
$$X \sim N(\theta, \lambda= \frac{1}{\sigma^2}) $$
$$ \theta \sim N(\mu, \nu=\frac{1}{\sigma_{\mu}^2})$$
$$X = (X - \theta) + \theta$$
$$(X-\theta)\sim N(0,\lambda)$$

$$X \sim N(0,\lambda) + N(\mu, \nu) \sim N(\mu, \frac{\lambda + \nu}{\lambda \nu})$$

b)
we know that:
$$m(x_{new}|x) = \int f(x_{new}| \theta) \pi(\theta | x) d\theta$$
Where $\pi(\theta | x)$ is the posterior distribution that is a $N(\mu_{\theta}^{'} , \lambda_{\theta}^{'} )$:
$$ \mu_{\theta}^{'} = w \cdot \mu + (1 - w)\cdot \bar x_n$$
$$ \lambda_{\theta}^{'} = \nu + N \lambda$$
and:

$$w = \frac{\nu}{\nu + N \lambda}$$
using the same trick we did before:

$$m(x_{new}|x) \sim N(\mu_{\theta}^{'}, \frac{\lambda_{\theta}^{'} + \lambda}{\lambda_{\theta}^{'} \cdot \lambda})$$
