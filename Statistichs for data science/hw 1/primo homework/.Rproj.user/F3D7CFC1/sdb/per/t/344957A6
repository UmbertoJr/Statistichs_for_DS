{
    "collab_server" : "",
    "contents" : "---\ntitle: 'Homework #1'\nauthor: |\n  | Stat4DS2+DS\n  | <https://elearning2.uniroma1.it/course/view.php?id=4951>\ndate: '**deadline 23/03/2017 (23:55)**'\noutput: \n  pdf_document:\n    toc: true\n---\n\n\n\n```{r setup, include=FALSE}\nknitr::opts_chunk$set(echo = TRUE, out.width='500px', dpi=200, fig.height = 4)\n```\n\n\nMy Last+First Name _Umberto_Jr_Mele_ My Matricola 1388371\n--------------\n\\newpage\n\n\n# Question 1\n1)  Sample survey: Suppose we are going to sample 100 individuals from\na county (of size much larger than 100) and ask each sampled person\nwhether they support policy Z or not. Let $Y_i = 1$ if person $i$ in the sample\nsupports the policy, and $Y_i = 0$ otherwise.\n\n## Part 1.a\na) Assume $Y_1,...,Y_{100}$ are, conditional on $\\theta$, i.i.d. binary random variables with expectation $\\theta$. Write down the joint distribution of \n$Pr(Y_1 =y_1, ... , Y_{100} = y_{100}| \\theta)$ \nin a compact form. \nAlso write down the form of\n$Pr(\\sum_{i=1}^n Y_i = y| \\theta)$.\n\n\n####-Answer:\n\n$Y_i|\\theta$ is a random variable with distribution $Bernoulli$ and $i.i.d.$ so we can write down the joint distribution, like this:\n\n$$Pr(Y_{1}=y_1,......, Y_{100}= y_{100} |\\theta) =P(Y_{1}=y_1|\\theta)P(Y_{2}=y_2|\\theta)...P(Y_{100}=y_100|\\theta) =\\prod_{i=1}^{100} \\theta^{y_i}(1 -\\theta)^{1-y_i}$$\nAnd we can say that this distribution is the Likelihood function on the parameter $\\theta$.\n\nWhile, the probability distribution of $Pr(\\sum_{i=1}^{100} Y_i = y |\\theta)$\nis defined by the random variable $Z=Y_1 + .... + Y_{100}$, that is distribuited like a Binomial distribution, so:\n$$Pr(\\sum_{i=1}^{100} Y_i = y |\\theta) = Pr(Z=y) = \\binom{100}{y} \\theta^y (1 - \\theta)^{100-y} $$\nNote that:\n$$Pr(\\sum_{i=1}^{100} Y_i = y |\\theta) \\propto  Pr(Y_{1}=y_1,......, Y_{100}= y_{100} |\\theta) = L(\\theta)$$\n\n\\newpage\n\n## Part 1.b\n\nb) For the moment, suppose you believed that \n$\\theta \\in \\{ 0.0, 0.1, ... , 0.9, 1.0 \\}$.\nGiven that the results of the survey \nwere $\\sum_{i=1}^n Y_i  = 57$ , \ncompute\n$$ Pr( \\sum_{i=1}^n Y_{i}  = 57 | \\theta)$$ \nfor each of these 11 values of $\\theta$ \nand plot these probabilities as a function of $\\theta$.\n\n####-Answer:\n\nThe given probability is:\n$$Pr(\\sum_{i=1}^{100} Y_i = 57 |\\theta) = Pr(Z=57) = \\binom{100}{57} \\theta^{57} (1 - \\theta)^{100-57} $$\n\n```{r}\ntheta <- seq(0,1,0.1)\nprob_Z.equal.57 <- dbinom(57 ,100, theta)\nplot(theta, prob_Z.equal.57, xlab = 'theta' , ylab = 'prob')\n```\n\n\\newpage\n\n## Part 1.c\n\nc) Now suppose you originally had no prior information to believe one of\nthese $\\theta$-values over another, and so \n$Pr(\\theta = 0.0) = Pr(\\theta = 0.1) = ... = Pr(\\theta = 0.9) = Pr(\\theta = 1.0)$. \nUse Bayes rule to compute $\\pi(\\theta|\\sum_{i=1}^n Y_i  = 57)$ \nfor each $\\theta$-value. Make a plot of this posterior distribution as a\nfunction of $\\theta$.\n\n####-Answer:\n\nRemembering that the posterior distribution is:\n$$\\pi(\\theta | x) = \\frac{L(\\theta) \\cdot \\pi(\\theta)}{m(x)}$$\n\nwe can compute in a similar way:\n$$\\pi(\\theta|\\sum_{i=1}^n Y_i  = 57)= \\frac{f(\\sum Y_i = y | \\theta) \\cdot \\pi(\\theta)}{\\sum_{\\theta} f(\\sum Y_i = y | \\theta) \\cdot \\pi(\\theta)}$$\n\n```{r}\nprior <- function(t)  1/11 *(t < 1 & t >0)\nL <-  function(t) dbinom(57,100,t)\nm <- function(all.theta){\n  su <- 0\n  for(t in all.theta){\n    su <- su + prior(t)*L(t)\n  }\n  return(su)\n}\njn <- function(t) prior(t)*L(t)\nposterior <- function(t) prior(t)*L(t)/m(all.theta = theta)\nplot(theta ,posterior(theta),type = 'p', col ='green', ylab='join and posterior')\nlines(theta ,jn(theta),type = 'p', col ='red')\ncurve(posterior(x), add = T, col='blue')\n\n```\n\n\n\\newpage\n\n## Part 1.d\n\nd) Now suppose you allow $\\theta$ to be any value in the interval $\\Theta=[0, 1]$. \nUsing the uniform prior density for $\\theta \\in [0,1]$, so that $\\pi(\\theta) = I_{[0,1]}(\\theta)$, \nplot $\\pi(\\theta) \\times Pr(\\sum_{i=1}^n Y_i  = 57|\\theta)$ as a function of $\\theta$.\n\n####-Answer:\n\n```{r}\nprior <- function(t) dunif(t)*(t>0 & t<1 )\nL <-  function(t) dbinom(57,100,t)\njoint <- function(t) L(t)*prior(t)\nm <- integrate(joint,0,1)\n\nposterior <- function(t) prior(t)*L(t)/m$value\ncurve(posterior, col = 'violet')\n\n```\n\n\n\\newpage\n\n## Part 1.e\n\ne) As discussed in this chapter, the posterior distribution of $\\theta$ is $Beta (1+57, 1+100-57)$. Plot the posterior density as a function of $\\theta$. Discuss the relationships among all of the plots you have made for this exercise.\n\n####-Answer:\n\nWe know that the posterior distribution is a $Beta(\\alpha_{post} = 1+57, \\beta_{post}=1 +43)$ and if we use the Conjugate Analysis for this problem, we have a $Beta$ distro as Prior:\n\n$$\\pi(\\theta | \\bar{Z}) \\propto L(\\theta) \\cdot\\pi(\\theta) = \\binom{100}{57} \\theta^{57} (1 - \\theta)^{100-57} \\cdot \\frac{\\theta^{\\alpha - 1}(1 - \\theta)^{\\beta - 1}}{B(\\alpha,\\beta)} \\propto \\theta^{57 + \\alpha-1} (1 - \\theta )^{43 + \\beta -1}$$\n\nso the posterior distribution is a $Beta$ distribution with parameters $\\dot\\alpha = 57 + \\alpha$ and $\\dot\\beta = 43 + \\beta$, so the prior distribution is a $Beta(1,1)$ that is the same distribution of a $Unif(0,1)$.\n\nBecause:\n$$Beta(\\theta | \\alpha = 1, \\beta=1) \\sim \\frac{\\theta^{\\alpha - 1}(1 - \\theta)^{\\beta - 1}}{B(\\alpha,\\beta)} = \\frac{\\theta^{1 - 1}(1 - \\theta)^{1 - 1}}{B(\\alpha,\\beta)} = constant \\quad \\forall \\theta \\in [0,1]$$\n\n\n```{r}\ncurve(dbeta(x,1,1))\n```\n\n\n```{r}\nprior <- function(t) dbeta(t,1,1)\nL <-  function(t) dbinom(57,100,t)\njoint <- function(t) L(t)*prior(t)\nm <- integrate(joint,0,1)\n\nposterior_3 <- function(t) prior(t)*L(t)/m$value\ncurve(posterior_3, col = 'green')\n```\n\nAnd we can concluding saying that we pose the same supposition for all priors we made (same propbability for all thetas we have), and the posterior distribution is the same of the Likelhood beacause our prior is not changing the distribution of the posterior.\n\n\n\n\\newpage\n\n# Question 2\n\n2) Consider a normal statistical model with $X_i \\sim N(\\theta,\\lambda=1/\\sigma^2)$ where the precision parameter is known. \nUse as a prior distribution on the (conditional) mean $\\theta$ a Normal with prior mean $\\mu$ and prior precison $\\nu$. \n\n## Part 2.a\n\na) derive the general formula of the prior predictive distribution \n\n####-Answer\n\nThe prior predictive distribution is defined by:\n$$m(\\cdot) = \\int f(\\cdot | \\theta) \\pi(\\theta) d\\theta = \\int \\frac{\\sqrt{\\lambda}}{\\sqrt{2\\pi}}e^{- \\frac{\\lambda(x - \\theta)^2}{2}} \\cdot \\frac{\\sqrt{\\nu}}{\\sqrt{2\\pi}}e^{- \\frac{\\nu(\\theta - \\mu)^2}{2}} d\\theta $$\n$$ \\propto  \\int e^{-\\frac{\\lambda(x^2- 2 \\theta x +\\theta^2)+ \\nu(\\theta^2 - 2\\theta \\mu + \\mu^2)}{2}}d\\theta \\quad \\propto e^{-\\frac{\\lambda x^2 + \\nu \\mu^2}{2}}\\int e^{-\\frac{(\\lambda + \\nu)\\theta^2}{2}+(\\lambda x +\\nu \\mu)\\theta}d\\theta $$\nNow remembering that:\n$$ N(\\mu=\\frac{b}{a} ; \\sigma^2=\\frac{1}{a}) \\propto exp\\{-\\frac{a x^2}{2} + bx\\}\n$$\nand..\n$$\\int \\frac{\\sqrt{a}}{\\sqrt{2\\pi}} \\cdot e^{-\\frac{ax^2}{2}+bx - \\frac{b^2}{2a}} dx = 1\n$$\nwe can compute:\n$$\\int e^{-\\frac{ax^2}{2}+ bx} = \\frac{\\sqrt{2\\pi}}{\\sqrt{a}} \\cdot e^{\\frac{b^2}{2a}}$$\nso:\n$$m(\\cdot) \\propto \\quad e^{-\\frac{\\lambda x^2 + \\nu \\mu^2}{2}}\\int e^{-\\frac{(\\lambda + \\nu)\\theta^2}{2}+(\\lambda x +\\nu \\mu)\\theta}d\\theta \\quad \\propto e^{-\\frac{\\lambda x^2 + \\nu \\mu^2}{2}} \\cdot e^{\\frac{(\\lambda x+ \\nu \\mu)^2}{2(\\lambda + \\nu)}}$$\n\n$$\\propto e^{-\\frac{\\lambda x^2}{2}} \\cdot e^{\\frac{\\lambda^2 x^2 + 2 \\lambda x \\nu \\mu}{2(\\lambda + \\nu)}} \\propto e^{-\\frac{1}{2}(\\lambda - \\frac{\\lambda^2}{\\lambda + \\nu})x^2 + \\frac{\\lambda\\nu\\mu}{\\lambda + \\nu}x}$$\nthe last function is a $N(a=(\\lambda - \\frac{\\lambda^2}{\\lambda + \\nu}) ; b =\\frac{\\lambda\\nu\\mu}{\\lambda + \\nu})$;\nso the prior predictive distribution is a normal: $N(\\frac{b}{a}= \\mu; \\frac{1}{a}=\\frac{\\lambda + \\nu}{\\lambda \\nu})$\n\n\nAnother way to compute this distribution is to think the distribution as a convolution:\n$$m(\\cdot) = \\int f(\\cdot | \\theta) \\pi(\\theta) d\\theta = \\int \\frac{\\sqrt{\\lambda}}{\\sqrt{2\\pi}}e^{- \\frac{\\lambda(x - \\theta)^2}{2}} \\cdot \\frac{\\sqrt{\\nu}}{\\sqrt{2\\pi}}e^{- \\frac{\\nu(\\theta - \\mu)^2}{2}} d\\theta = \\int W(x - \\theta)\\cdot \\pi(\\theta) d\\theta$$\nwhere $W(w)$ is a Normal $N(0;\\frac{1}{\\lambda})$.\n\nSo for the properties of convolution we know that:\n\n$$X = W + \\theta$$\nwhere $W$ and $\\theta$ must be independent and both Gaussians.\n\n$$ \\theta \\sim N(\\mu_{\\pi}, \\sigma_{\\pi}^2=\\frac{1}{\\nu_{\\pi}^2})$$\n\n$$W\\sim N(0,\\sigma^2=\\frac{1}{\\lambda})$$\n\n$$X \\sim N(0,\\lambda)\\ast N(\\mu, \\nu) \\sim N(\\mu_{pred}=\\mu_{\\pi},\\sigma_{pred}^2 = \\frac{\\lambda + \\nu}{\\lambda \\nu})$$\nwhere:\n$$\\mu_{pred} = E(X) = E(W + \\theta)= 0 + \\mu_{\\pi}$$\nand for the independence:\n\n$$\\sigma_{pred}^2 = Var(X)=Var(W+\\theta)=Var(W)+Var(\\theta)=\\sigma_{\\pi}^2 + \\sigma^2= \\frac{\\lambda + \\nu}{\\lambda \\nu}$$\n\n\\newpage\n\n## Part 2.b\n\nb) derive the general formula of the posterior predictive distribution \n\n##-Anwer\n\nRemembering that:\n\n$$m(x_{new}|\\hat x) = \\int f(x_{new}| \\theta) \\pi(\\theta |\\hat  x) d\\theta$$\n\nwe know that $\\pi(\\theta | x)$ is the posterior distribution that is a $N(\\mu_{\\theta}^{'} , \\lambda_{\\theta}^{'} )$:\n\n$$ \\mu_{\\theta}^{'} = w \\cdot \\mu + (1 - w)\\cdot \\bar x_n$$\n\n$$ \\nu_{\\theta}^{'} = \\nu + n \\lambda$$\nand:\n\n$$w = \\frac{\\nu}{\\nu + n \\lambda}$$\n\nso it's easy now to compute the posterior predictive distribution:\n\n$$m(x_{new}|x) = \\int f(x_{new}| \\theta) \\pi(\\theta | x) d\\theta = \\int \\frac{\\sqrt{\\lambda}}{\\sqrt{2\\pi}}e^{- \\frac{\\lambda(x - \\theta)^2}{2}} \\cdot \\frac{\\sqrt{\\nu_{\\theta}^{'}}}{\\sqrt{2\\pi}}e^{- \\frac{\\nu_{\\theta}^{'}(\\theta - \\mu_{\\theta}^{'})^2}{2}} d\\theta$$\n$$\\propto e^{-\\frac{1}{2}(\\lambda - \\frac{\\lambda^2}{\\lambda + \\nu_{\\theta}^{'}})x^2 + \\frac{\\lambda\\nu_{\\theta}^{'}\\mu_{\\theta}^{'}}{\\lambda + \\nu_{\\theta}^{'}}x}$$\nbecause is the same integral we compute before.\n\nSo the posterior predictive distribution is a $N(a=(\\frac{\\lambda\\nu_{\\theta}^{'}}{\\lambda + \\nu_{\\theta}^{'}}) ; b =\\frac{\\lambda\\nu_{\\theta}^{'}\\mu_{\\theta}^{'}}{\\lambda + \\nu_{\\theta}^{'}})=N(\\mu =\\mu_{\\theta}^{'},\\sigma^2 = \\frac{\\lambda + \\nu_{\\theta}^{'}}{\\lambda \\nu_{\\theta}^{'}})$\n\nWe could arrive to the same answer also with convolution method we saw before.\n\n\\newpage\n\n## Part 2.c\n\nc) assume that the known value of $\\lambda$ is $1/3$ and suppose you have observed the following data \n$$\n-1.25 \\,\\,\\,\n8.77 \\,\\,\\,\n1.18 \\,\\,\\,\n10.66  \\,\\,\\,\n11.81  \\,\\,\\,\n-6.09   \\,\\,\\,\n3.56  \\,\\,\\,\n10.85   \\,\\,\\,\n4.03   \\,\\,\\,\n2.13 \\,\\,\\,\n$$\nElicit your prior distribution on the unknown $\\theta$ in such a way that your prior mean is 0 and you believe that the unknown theta is in the interval $[-5,5]$ with prior probability 0.96\n\n###-Answer\n\nWe want that $P_{\\pi}(-5 \\leq \\theta \\leq 5) = 0.96$, and $\\mu = 0$ so:\n\n$$P_{\\pi}(-5 \\leq \\theta \\leq 5) = P_{\\pi}(\\frac{-5 - \\mu}{\\sigma} \\leq\\frac{\\theta - \\mu}{\\sigma} \\leq \\frac{5 - \\mu}{\\sigma}) = P_{\\pi}(\\frac{-5}{\\sigma} \\leq\\ Z \\leq \\frac{5}{\\sigma})= \\Phi( \\frac{5}{\\sigma}) - \\Phi(- \\frac{5}{\\sigma})= 1 - 2\\Phi(-\\frac{5}{\\sigma})$$\n$$\\Phi(- \\frac{5}{\\sigma})=0.02$$\n\n```{r}\nqnorm(0.02)\n```\n\n$$\\sigma = \\frac{-5}{-2.053749}=2.4346$$\n\nso the prior is a $N(\\mu= 0,\\sigma^2 =5.93)$ or in an other parametrization $N(\\mu=0, \\nu = 0.1687)$.\n\n\\newpage\n\n## Part 2.d\n\nd) derive your posterior distribution and represent it graphically \n\n###-Answer\n\nSo:\n$$\\pi(\\theta | \\bar X) \\propto L(\\theta) \\cdot \\pi(\\theta) \\propto \\prod_{i=0}^{10} f(x |\\theta)  \\cdot \\pi(\\theta)$$\n$$\\propto \\prod_{i=0}^{10} \\frac{\\sqrt{\\lambda}}{\\sqrt{2\\pi}} \\cdot e^{-\\frac{\\lambda(x_i - \\theta)^2}{2}} \\cdot  \\frac{\\sqrt{\\nu}}{\\sqrt{2\\pi}} \\cdot e^{-\\frac{\\nu(\\theta - \\mu)^2}{2}} \\propto e^{-\\frac{\\lambda}{2}\\sum_{i=0}^{10}(x_i - \\theta)^2} \\cdot e^{-\\frac{\\nu(\\theta - \\mu)^2}{2}}$$\n$$\\propto e^{-\\frac{\\lambda}{2}[\\sum(x_i^2 - 2\\theta x_i) + n\\theta^2] - \\frac{\\nu}{2}(\\theta^2 - 2\\mu\\theta + \\mu^2)} \\propto e^{-\\frac{\\lambda}{2}(-2 \\theta \\sum x_i + n\\theta^2) - \\frac{\\nu}{2}(\\theta^2 -2\\mu\\theta)} \\propto e^{-\\frac{1}{2}(n\\lambda + \\nu)\\theta^2+ (\\nu\\mu + \\lambda \\sum x_i)\\theta}$$\nSo the distribution is a \n$$N(a_{post}=(n\\lambda + \\nu); b_{post}=(\\nu\\mu + \\lambda \\sum x_i))$$ where:\n$$N(\\mu_{post}=\\frac{b_{post}}{a_{post}}; \\nu_{post}=a_{post})$$\n\n```{r}\npar(mfrow = c(2,2))\nX.bar <- c(-1.25, 8.77, 1.18, 10.66, 11.81, -6.09, 3.56, 10.85, 4.03, 2.13)\n\nL1 <- function(thet){\n  prod=1\n  for(x in X.bar){\n    prod = prod*dnorm(x, mean=thet, sd=sqrt(3))\n  }\n  return(prod)\n}\n\n#int.1 <- integrate(L1,-Inf,Inf)\n#prob1<-function(x) L1(x)/int.1$value\ncurve(L1, xlab = expression(theta), ylab = expression(L(theta)), from = 0, to = 10)\n\npr <- function(thet) dnorm(thet, 0, (-5/qnorm(0.02)))\ncurve(pr, ylab = 'prior', xlab=expression(theta), from = -10, to = 10)\n\njnt1 <- function(t) L1(t)*pr(t)\nm1.x_bar <- integrate(jnt1, -Inf, Inf)\npost1 <- function(t) jnt1(t)/m1.x_bar$value\n\ncurve(post1, from = 0, to = 10, ylab = expression(posterior), \n      xlab = expression(theta), col='green')\n```\n\n\\newpage\n\n### Part 2.e\n\ne) derive your favorite point estimate and interval estimate and motivate your choices\n\n####-Answer\n\nThe posterior distribution of $\\theta$ is in a Bayesian way to think, the real density function of $\\theta$, so\nwe have a good point estimator for our parameter using the expected value of our $\\theta$:\n$$E(\\theta) = \\int \\theta \\cdot \\pi(\\theta | \\bar x) d\\theta$$\nand in this case where we have a normal distribution like posterior\nby computing the mean of our posterior distribution:\n$$ \\mu_{\\theta}^{'} = w \\cdot \\mu + (1 - w)\\cdot \\bar x_n$$\n$$w = \\frac{\\nu}{\\nu + n \\lambda} = \\frac{ 0.1687 }{0.1687 + 10\\cdot 0.\\bar 3} = 0.048$$\nso:\n\n$$\\mu_{\\theta}^{'} = 0.044 \\cdot 0 + (1 - 0.048) \\cdot 4.565 = 4.345$$\n\nWhile to compute the credible interval we must know the variance of our posterior, so:\n\n\n$$ \\nu_{\\theta}^{'} = \\nu + n \\lambda= 0.1687 + 10\\cdot \\frac{1}{3} = 3.50$$\nand:\n\n$$\\sigma_{\\theta}^{'} =\\sqrt \\frac{1}{\\nu_{\\theta}^{'}}= 0.535$$\n\nso:\n$$P(\\mu_{\\theta}^{'} -z_{\\alpha/2}\\cdot \\sigma_{\\theta}^{'} < \\theta|x < \\mu_{\\theta}^{'} +z_{\\alpha/2}\\cdot \\sigma_{\\theta}^{'}  ) = 1- \\alpha$$\nfor a confidence interval with probabilty of $0.96$ we have:\n\n```{r}\nnu = 0.1687154\nw = nu/(nu+ 10/3)\nmu = (1-w)*mean(X.bar)\nnu.tet = nu + 10/3 \nsig= sqrt(1/nu.tet)\nz = qnorm(0.02)\ncat('mean =',mu,'\\n')\nc(lower = mu + z*sig, upper= mu - z*sig)\n\n```\n\n\\newpage\n\n# Question 3\n\n3) As an alternative model for the previous 10 observations \n$$\n-1.25 \\,\\,\\,\n8.77 \\,\\,\\,\n1.18 \\,\\,\\,\n10.66  \\,\\,\\,\n11.81  \\,\\,\\,\n-6.09   \\,\\,\\,\n3.56  \\,\\,\\,\n10.85   \\,\\,\\,\n4.03   \\,\\,\\,\n2.13 \\,\\,\\,\n$$\nconsider the following statistical model where $X_i|\\theta$ are i.i.d with \n$$\nX_i|\\theta \\sim f(x|\\theta) = \\frac{1}{20} I_{[\\theta-10,\\theta+10](x)}\n$$\nUse the same prior elicitation for $\\theta$ as in the model of the previous excercise \n\n## Part 3.a\n\na) Provide a fully Bayesian analysis for these data explaining all the basic ingredients and steps for carrying it out. In particular,  compare your final inference on the uknown $\\theta=E[X|\\theta]$ with the one you have derived in the previous point 2) \n\n####-Answer\n\nSo, remembering that:\n$$\\pi(\\theta | \\bar X) \\propto L(\\theta) \\cdot \\pi(\\theta) \\propto \\prod_{i=0}^{10} f(x |\\theta)  \\cdot \\pi(\\theta)$$\nwe have that:\n$$L(\\theta) = \\prod_{i=0}^{10} f(x |\\theta) = \\prod_{i=0}^{10} \\frac{1}{20} I_{[\\theta-10,\\theta+10](x_i)}$$\nand we can say that:\n$$\\begin{cases} L(\\theta)=0 & if  \\quad\\theta<\\theta_{min} \\\\ L(\\theta)=\\frac{1}{20^{10}} & if \\quad \\theta_{min} \\leq \\theta \\leq \\theta_{max} \\\\ L(\\theta)=0 & if \\quad\\theta \\geq \\theta_{max} \\end{cases}$$\nwhere:  $\\theta_{min}=max(\\bar X)-10$ and $\\quad \\theta_{max}=min(\\bar X)+10$\n\n\n```{r}\ntheta.min <- max(X.bar) - 10\ntheta.max = min(X.bar) + 10\nc(min = theta.min, max = theta.max)\n```\n\n\n\n\nSo:$$L(\\theta) \\propto U(1.81,3.91)$$\n\nOur prior is: $\\pi(\\theta)=N(\\mu=0, \\nu = 0.1687)$\n\nSo, finally we can derive the posterior distribution:\n$$\\pi(\\theta | \\bar X)\\propto L(\\theta) \\cdot \\pi(\\theta) \\propto \\frac{1}{20^{10}} \\cdot I_{(\\theta \\in [1.81 ; 3.91])} \\cdot \\frac{\\sqrt{\\nu}}{\\sqrt{2\\pi}}e^{-\\frac{\\nu}{2}\\theta^2} \\propto  \\frac{\\sqrt{\\nu}}{\\sqrt{2\\pi}}e^{-\\frac{\\nu}{2}\\theta^2}\\cdot I_{(\\theta \\in [1.81 ; 3.91])} $$\nThat is a truncated Normal.\n\n\\newpage\n\n\n\n```{r}\nX.bar <- c(-1.25, 8.77, 1.18, 10.66, 11.81, -6.09, 3.56, 10.85, 4.03, 2.13)\n\nL2 <- function(thet){\n  prod=1\n  for(x in X.bar){\n    prod = 1/12*prod*(x>=thet-10 & x<=thet+10)\n  }\n  return(prod)\n}\n\nint2 <- integrate(L2,-Inf,Inf)\nprob2 <- function(x) L2(x)/int2$value\ncurve(prob2, xlab = 'theta', ylab = 'L(theta)', from = 0, to = 10)\n```\n\n\n\\newpage\n\n\n```{r}\npr <- function(thet) dnorm(thet, 0, (-5/qnorm(0.02)))\n\n\njnt2 <- function(t) L2(t)*pr(t)\nm2.x_bar <- integrate(jnt2, -Inf, Inf)\npost2 <- function(t) jnt2(t)/m2.x_bar$value\ncurve(post2, from = -10, to = 10, col='blue',ylab = 'Prior and Posterior')\ncurve(pr, ylab = 'prior', xlab='theta', add = T)\n```\nAt last, comparing the posterior distribution of previous model and this last model, we can infer that the first model allows all possible thetas but mainly in the interval $[3.25; 5.44]$ with expected value equal $4.35$, instead this last model recommends a finite interval of thetas that has to be in the interval $[1.81;3.91]$ with probability 1, and with expected value $2.76$\n\n\n\n\n```{r}\nmean.int <- function(x) x*post2(x)\nE <- integrate(mean.int, -Inf, Inf)\nE\n```\n\n\\newpage\n\n## Part 3.b\n\n\nb) Write the formula of the prior predictive distribution of a single observation and explain how you can simulate i.i.d random drws from it. Use the simulated values to represent approximately the predictive density in a plot and compare it with the prior predictive density of a single observation  of the previous model\n\n\n####-Answer\n\nLike always the prior predictive distribution is:\n$$m(\\cdot) = \\int f(\\cdot | \\theta) \\pi(\\theta) d\\theta$$\nso:\n$$m(x)= \\int \\frac{1}{20} \\cdot I_{(\\theta \\in [x - 10; x + 10])} \\frac{\\sqrt\\nu}{\\sqrt{2\\pi}}e^{-\\frac{\\nu}{2}\\theta^2}d\\theta = \\int_{x-10}^{x+10} \\frac{1}{20}\\cdot \\frac{\\sqrt\\nu}{\\sqrt{2\\pi}}e^{-\\frac{\\nu}{2}\\theta^2}d\\theta$$\n$$= \\frac{1}{20} \\cdot P(x-10< \\theta_{prior} < x +10)= \\frac{1}{20}(\\Phi(\\frac{x + 10}{\\sigma})-\\Phi(\\frac{x - 10}{\\sigma})) = \\frac{1}{20}(\\Phi(\\frac{x + 10}{2.43})-\\Phi(\\frac{x - 10}{2.43}))$$\n\n\n```{r}\nx <- seq(-30,30,0.1)\nmarginal <- function(x) 1/20*(pnorm((x+10)*sqrt(nu)) - pnorm((x-10)*sqrt(nu)))\n\nx.prob <- marginal(x)\nplot(x,x.prob, type= 'l', col='blue',ylab='m(x)')\n\n```\nAnd we can see this distribution looks like an uniform distribution but is not.\n\nWe can compute also an approssimation for this distribution function using a simulation with the Acceptance-rejection method;\n\n```{r}\nmarginal <- function(x) 1/20*(pnorm((x+10)*sqrt(nu)) - pnorm((x-10)*sqrt(nu)))\nbound <- function(x) 0.6 *(x>-30 & x<30)\nobs <- c()\nfor(i in 1:100000){\n  y <- runif(1,-25,25)\n  z <- rbinom(1,1,marginal(y)/0.6)\n  if(z==1){\n    obs=c(obs,y)\n  }\n} \nhist(obs, probability = T)\ncurve(marginal(x), type= 'l',add = T, col='blue')\nd <- density(obs)\nlines(d,type= 'l', col='green')\nlegend(8.3,0.05,c('prior predictive','Kernel of obs'), lty = c(1,1),\n       col=c('blue','green'), lwd = c(2.5,2.5),cex = 0.75)\n```\n\n\n\n\nWhile the pior predictive distribution for the previous model is:\n$N(0,8.927)$\n```{r}\nplot(x, dnorm(x,sd=sqrt(8.927)),type = 'l',col='blue',ylab = 'Prior predictive distribution')\n```\nThis prior predictive distribution has higher probability for x close to 0 and fewer to values further, and don't have a behaviour similar to a Uniform distribution of course.\n\n\n\\newpage\n\n\n## Part 3.c\n\nc) Consider the same discrete (finite) grid of values as parameter space $\\Theta$ for the conditional mean $\\theta$ in both models. Use this simplified parametric setting to decide whether one should use the Normal model rather than the Uniform model in light of the observed data. \n\n####-Answer \n\nUsing the properties of Bayesian multimodel inference, we have that:\n\n$$J(m,\\theta,data) = prior(m)\\cdot\\pi(\\theta|m)\\cdot f(data|\\theta,m)$$\nand:\n$$J(data,m)= \\int J(m,\\theta,data) d\\theta =prior(m)\\cdot b(m|data)$$\nwhere $b(m|data)= J(data|m)=\\int \\pi(\\theta|m)\\cdot f(data|\\theta, m) d\\theta$.\n\nSo to compute the posterior distribution, we should:\n\n$$post(m|data)= \\frac{prior(m)\\cdot b(m|data)}{\\sum_{m_{i}}prior(m_{i})\\cdot b(m_{i}|data)}$$\n\nFinally we cann compare both models using:\n\n$$\\frac{post(m_1|data)}{post(m_2|data)}= \\frac{prior(m_1)}{prior(m_2)}\\cdot \\frac{b(m_1|data)}{b(m_2|data)}$$\n\nand using the same prior probabilty for both models, we have that:\n\n$$\\frac{post(m_1|data)}{post(m_2|data)}= \\frac{b(m_1|data)}{b(m_2|data)}$$\nthat is the Bayes Factor!!!\n\n$$BF=\\frac{\\int \\prod_{i=0}^{10} \\frac{\\sqrt{\\lambda}}{\\sqrt{2\\pi}} \\cdot e^{-\\frac{\\lambda(x_i - \\theta)^2}{2}} \\cdot  \\frac{\\sqrt{\\nu}}{\\sqrt{2\\pi}} \\cdot e^{-\\frac{\\nu(\\theta - \\mu)^2}{2}}d\\theta }{\\int \\frac{1}{20^{10}} \\cdot I_{(\\theta \\in [1.81 ; 3.91])} \\cdot \\frac{\\sqrt{\\nu}}{\\sqrt{2\\pi}}e^{-\\frac{\\nu}{2}\\theta^2}d\\theta}$$\n\n\n```{r}\nbayes.factor <- m1.x_bar$value/m2.x_bar$value\nbayes.factor\n```\n\nAt the end, we can see that computing the BF, we infer that the __second model__ is a better choice.\n\n\n\n\\addvspace{0.5cm}\n\n* * *\n<div class=\"footer\"> &copy; 2016-2017 - Stat4DS2+CS - Luca Tardella </div>\n\n```{r, warning=FALSE, error=FALSE, message=FALSE, echo=FALSE}\ncat(paste0(\"This homework will be graded and it will be part of your final evaluation \\n\\n \"))\ncat(paste(\"Last update by LT:\",date()))\n```\n\n\n",
    "created" : 1489583258439.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "246429366",
    "id" : "344957A6",
    "lastKnownWriteTime" : 1490226531,
    "last_content_update" : 1490226531190,
    "path" : "C:/Users/Umbertojunior/Desktop/data science/Second Semestr/SDS 2/hw 1/primo homework/Homework-1_pdf.Rmd",
    "project_path" : "Homework-1_pdf.Rmd",
    "properties" : {
        "last_setup_crc32" : "BA1DCFB459447ab3"
    },
    "relative_order" : 2,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_markdown"
}