{
    "collab_server" : "",
    "contents" : "#########################################################\n# Markov Chain simulation on a discrete state space\n# with only two states\n# S={1,2}\n#########################################################\n\nS=c(1,2)        # discrete state space\n\n\nalpha<-0.2      # set up the TPM (Transition Probability Matrix)\nbeta<-0.5\ntpm<-matrix(c(1-alpha,alpha,beta,1-beta),nrow=2,byrow=T)\n\ntpm\n\n\nx1<-2 ## starting value of the chain\n## [from a degenerate distribution with mass 1 at state s=2]\n\nnsample<-10\nchain<-rep(NA,nsample+1) # vector that will hold\n# all the simulated values\nchain[1]<-x1             # starting value x1 assigned to chain[1]\nfor(t in 1:nsample){\n  chain[t+1]<-sample(S,size=1,prob=tpm[chain[t],])\n}\n\nS\nplot(chain,ylim=c(0,4))\nplot(chain,ylim=c(0,4),type=\"b\")\ntable(chain)\n\nprop.table(table(chain))\n\nmean(chain)\n\n\n### Let us start providing some evidence of stability of the behaviour of the initial stream of one sample path of the Markov chain of suitable length\n\nnsample<-10000 #  length of the initial part of the sample path \n#  of our Markov chain (stochastic process) \n\nchain<-rep(NA,nsample+1) # vector that will hold\n# all the simulated values\n\n# Let us start from x1=1\n\nx1=1\nchain[1]<-x1             # starting value x1 assigned to chain[1]\nfor(t in 1:nsample){\n  chain[t+1]<-sample(S,size=1,prob=tpm[chain[t],])\n}\n\nmean(chain)       # our first example of empirical mean [h(x)=x]\nmean(chain==2)    # our second example of empirical mean [h(x)=I_{1}(x)]\ntable(chain)      \nprop.table(table(chain))\n\n\n\n# Let us start the chain from another value x1=2\n\nx1=2\nchain[1]<-x1             # starting value x1 assigned to chain[1]\nfor(t in 1:nsample){\n  chain[t+1]<-sample(S,size=1,prob=tpm[chain[t],])\n}\n\nmean(chain)       # our first example of empirical mean [h(x)=x]\nmean(chain==2)    # our second example of empirical mean [h(x)=I_{1}(x)]\ntable(chain)      \nprop.table(table(chain))\n\n# we experienced **approximately** the same empirical mean no matter what is the initial POINT we start the chain from!! [initial distribution can be regarded as and is in fact degenerate at that point!!]\n\n# Let us look at the stabilization of the empirical mean as long as the number of sampled states increases\n\nrunningmeans=cumsum(chain)/(1:length(chain))\nplot(1:length(chain),runningmeans)\ntitle(main=\"stabilization of the running means\")\nrunningmeans[length(runningmeans)]\n\n\n# Let us look at the stabilization of the empirical mean of the indicator function of state s=1 (relative frequency) as long as the number of sampled states increases\n\nstate=1\nrunningmeans=cumsum(chain==state)/(1:length(chain))\nplot(1:length(chain),runningmeans)\ntitle(main=\"stabilization of the running relative frequencies \\n of occurrence of state 1\")\nrunningmeans[length(runningmeans)]\n\n# now let us compare this value with the following entries \n# and try to interpret what is going on \n\ntpm\ntpm%*%tpm\ntpm%*%tpm%*%tpm\ntpm%*%tpm%*%tpm%*%tpm\ntpm%*%tpm%*%tpm%*%tpm%*%tpm%*%tpm\n# ...\ntpm%*%tpm%*%tpm%*%tpm%*%tpm%*%tpm%*%tpm%*%tpm%*%tpm%*%tpm%*%tpm%*%tpm\ntpm%*%tpm%*%tpm%*%tpm%*%tpm%*%tpm%*%tpm%*%tpm%*%tpm%*%tpm%*%tpm%*%tpm%*%tpm%*%tpm%*%tpm%*%tpm%*%tpm%*%tpm%*%tpm%*%tpm%*%tpm%*%tpm%*%tpm%*%tpm\n\n\n\n\n\n\n\n\n#########################################################\n# Markov Chain simulation on a continuous state space\n# AR(1) process [random walk]\n#########################################################\n\n# X_{t+1} = rho * X_{t+1} + B_{t+1}\n# B_1,...,B_{t+1} i.i.d. N(0,1)\n\n# ==> X_{t+1} ~ N(rho*X_{t},sd=1)\n\nset.seed(123)\nmcsize=10000\n# Setup of the Markov transition kernel correponding \n# to the distribution of X_{t+1}|X_{t}=present\n\nrho=1.002\nrho=-0.2\nrho=0.9\n\nstarting_point=present=3.9\nchain=c(starting_point)\n\n# initialization of the chain at time t=0\n\n# loop for sequential updating from present to the next future time \n# transition kernel (updating) \n# made of a gaussian random draw where the gaussian distribution\n# has a mean depending on the present state\n\nprint(present)\n\nfor(i in 1:mcsize){\n  \n  # future=rnorm(1,mean=0.2*present,sd=sqrt(1))\n  # future=rnorm(1,mean=present)\n  # future=rnorm(1,mean=1.05*present)\n  # future=rnorm(1,mean=2*present)\n  \n  # alternative kernels depending on rho\n  # rho=0.9\n  \n  future=rnorm(1,mean=rho*present,sd=sqrt(1))\n  present=future\n  chain=c(chain,present)\n  # print(present)\n  \n}\n\n# final print of the realization of the path up to time t (t=mcsize)\n# (realization of the initial portion of the chain)\n\nplot(chain)\n\nplot(chain,type=\"b\")\n\niterationvec=seq(1,length(chain))\n\nplot(iterationvec,chain,type=\"l\",main=\"Trace plot\", sub=\"AR(1) - Random walk\",xlab=\"t\",ylab=\"chain state at time t\")\n\nhist(chain,freq=F)\ncurve(dnorm(x,sd=1/sqrt(1-rho^2)),col=\"red\",add=T)\n\n# \n\nacf(chain)\nacf(chain)$acf[2]\n\n# This continuous case example teaches us that things can go well eventually when n increases but not necessarily .... depending on ... what?\n\n",
    "created" : 1495988319506.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "17338287",
    "id" : "22086E81",
    "lastKnownWriteTime" : 1495386467,
    "last_content_update" : 1495386467,
    "path" : "C:/Users/Umbertojunior/Desktop/data science/Second Semestr/SDS 2/hw 2/re giorgio/Markov_chain_simulation_Tardella.R",
    "project_path" : null,
    "properties" : {
        "tempName" : "Untitled1"
    },
    "relative_order" : 5,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_source"
}