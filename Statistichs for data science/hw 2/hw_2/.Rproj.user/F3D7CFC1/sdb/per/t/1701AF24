{
    "collab_server" : "",
    "contents" : "# last update: 04.20.2017\n# (update n.obs after reading real data set!) \n\n# from Carlin, Gelfand e Smith (1992) \n# Applied Statistics, 41, 389-405\n# Hierarchical Bayesian Analysis of Change-Point Problems\n\n# BEST PRACTICE:\n# before implementing a GS to perform your Bayesian real data analysis\n# with a \"new\" model\n# \n# do first simulate synthetic data from the model (say with n.obs=100) \n# with known TRUE parameter values\n# and make inference on the simulated data \n\n# INGREDIENTS:\n\n# TRUE (fixed, known) parameters\n\nset.seed(1234)\nn.obs=100\nm.true=30\nlambda.true=1.5\nphi.true=3.5\n\n# synthetic data\n\n# from a *KNOWN* MODEL \n\ny.firstperiod=rpois(n=m.true,lambda=lambda.true)\ny.secondperiod=rpois(n=n.obs-m.true,lambda=phi.true)\n\ny=c(y.firstperiod,y.secondperiod)\n\nplot(y,xlab=\"t\",type=\"h\")\ntitle=(main=\"Simulated data\")\n\npoints(1:m.true,y.firstperiod,pch=\"x\",col=\"red\",font=2)\npoints((m.true+1):n.obs,y.secondperiod,pch=\"+\",col=\"blue\",font=2)\n\n# NOTICE WE CAN color THIS PLOT because WE HAVE SIMULATED OUR DATA!!\n\n# We would like to approximate (among other things)\n# (posterior) expectations of \n\n# lambda\n\n# phi \n\n# m\n\n\n\n# full conditionals \n\n# CAREFUL! parameterization of the gamma density in R \n\n# lambda|y,phi,m ~ Gamma(ip.alpha.rate+m,ip.beta.shape+sum(y))\n# phi|y,m,lambda ~ Gamma(ip.a.rate+n-m,ip.b.shape+sum(y))\n# m|y,lambda,phi ~ exp(log(lambda)*(newbeta.shape+sum(y[1:m])-1) -lambda*(newalpha.rate+m)\n#                      + log(phi)*(ip.b.shape+sum(y[(m+1):n.obs])-1) - phi*(b+n-m)         )  m=1,...,n-1\n\n# fix some (prior) values on the hyperparameters (parameter of the prior distribution)\n\nip.alpha.rate=0.1\nip.beta.shape=0.2\nip.a.rate=0.1\nip.b.shape=0.7\n\n# what uncertainty on the parameters?\n\n\n# fix the length of the simulated chains\n# and then \n# initialize the corresponding vectors\n\nn.iter=500\n\nlambda=rep(NA,n.iter+1)\nphi=rep(NA,n.iter+1)\nm=rep(NA,n.iter+1)\n\nm.support=seq(1,n.obs-1)\n\n# initialize the chain at time t=0 [1]\n\nlambda.start=lambda[1]=1.5\nphistart=phi[1]=3.5\nm.start=m[1]=30\n\n# is it fair? maybe ... well, of course not ... but this is certainly a good starting point!\n\n# otherwise \n\nlambda.start=lambda[1]=5\nphistart=phi[1]=7\nm.start=m[1]=60\n\n\nstat.y.firstperiod=cumsum(y)[-length(y)]\nstat.y.secondperiod=sum(y)-stat.y.firstperiod\n# for all m in m.support\n\nfor(t in 1:n.iter){\n  \n  # UPDATE POISSON PARAMETER FOR THE FIRST PERIOD \n  \n  lambda[t+1]=rgamma(1,shape=ip.beta.shape+sum(y[1:m[t]]),rate=ip.alpha.rate+m[t])\n  # lambda[t+1]=rgamma(1,shape=ip.beta.shape+stat.y.firstperiod[m[t]],rate=ip.alpha.rate+m[t])\n  \n  # UPDATE POISSON PARAMETER FOR THE SECOND PERIOD \n  \n  \n  phi[t+1]=rgamma(1,shape=ip.b.shape+sum(y[(m[t]+1):n.obs]),rate=ip.a.rate+n.obs-m[t])\n  # phi[t+1]=rgamma(1,shape=ip.b.shape+stat.y.secondperiod[m[t]],rate=ip.a.rate+n.obs-m[t])\n  \n  # UPDATE changepoint \n  # using the full-conditional (discrete) masses\n  \n  # CAREFUL!! we need to rescale on a proper range in order to avoid overflows\n  # related to exponentials\n  # (use log-scale!)\n  \n  logci=log(lambda[t+1])*(stat.y.firstperiod)- lambda[t+1]*(m.support)+ log(phi[t+1])*(stat.y.secondperiod)- phi[t+1]*(n.obs-m.support)\n  \n  # back to the natural scale\n  m.full.conditional.nn=exp(logci-max(logci))\n  \n  # NOTE that sample(...) does not need probability masses but *positive* masses (up to proportionality constant)\n  \n  m[t+1]=sample(x=m.support,size=1,prob=m.full.conditional.nn)\n  \n  \n}\n\njointchain=data.frame(lambda,phi,m)\n\n\n# FIRST OF ALL: TRACE PLOTS to get a feeling of the influence of the starting point and possible impact of autocorrelation \n\nplot(lambda,type=\"l\")\nacf(lambda)\n\nplot(phi,type=\"l\")\nacf(phi)\n\nplot(m,type=\"l\")\nacf(m)\n\n# THE (MCMC) SUMMARIES\n\nplot(table(m[]))\n\n# plot(table(m[25000:50001]))\n\n\n### Now let us explore the result of a simulation \n### from a Markov Chain with stationary distribution \n### corresponding to the (joint) posterior distribution \n### of the parameter of our change-point statistical model of interest\n\npar(mfrow=c(2,2))\nhist(m)\nplot(lambda,phi)\nplot(table(m),type=\"h\")\n\n\n# NOW LET US PROVIDE SOME INFERENTIAL FINDINGS WITH OUR BAYESIAN ANALYSIS AND COMPARE THEM WITH THE TRUE MODEL PARAMETERS!!\n\n\n# one way of summarizing the marginal posterior of m|data\nprop.table(table(m))\n\nnames(which.max(prop.table(table(m))))\n\n# alternatively\nmean(m)\n\n\n\n# TWO IMPORTANT THINGS \n\n# Don't forget to (possibly) remove some of the initial T_0 simulations\n\n# Don't forget to assess posterior uncertainty \n\n\n# WHAT ABOUT THE OTHER TWO PARAMETERS?\n\n\nsummary(lambda)\nmean(lambda)\nquantile(lambda,prob=c(0.025,0.975))\n\nquantile(lambda,seq(0.1,0.9,0.1))\n\n# SIMILARLY FOR phi\n\nsummary(phi)\nmean(phi)\nquantile(phi,prob=c(0.025,0.975))\n\nquantile(phi,seq(0.1,0.9,0.1))\n\n# OTHER FE\n\n# axis(1,at=seq(2,100,2),cex=0.5,las=2)\nhist(lambda/phi)\n\nmean(lambda/phi)\n\ncor(lambda,phi)\n\n\n\n# what do you expect if I change (increase) n.obs ?\n\n# what kind of posterior summaries would you report?\n\n\n\n\n\n\n# now *you* can use the real data to carry on your first approximate MCMC Bayesian inference!!!\n\ndisasters=read.csv(\"coal-mining-disasters.csv\")\nstr(disasters)\n\n\nplot(disasters$year,disasters$n.disasters,main=\"Number of fatal accidents in UK coal mining sites\\n[real data]\",xlab=\"year\",ylab=\"number of accidents\",type=\"h\")\n\n# recycle your previous code but .... careful!!\n\ny=disasters$n.disasters\nn.obs=length(y)\n\nm.support=seq(1,n.obs-1)\nplot(y,type=\"h\")\n\nset.seed(1234)\n\n# re-use the previous code ......",
    "created" : 1495638235187.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "4210635570",
    "id" : "1701AF24",
    "lastKnownWriteTime" : 1495638252,
    "last_content_update" : 1495638252826,
    "path" : "C:/Users/Umbertojunior/Desktop/data science/Second Semestr/SDS 2/hw 2/hw_2/second_example_gibbs_sampling.R",
    "project_path" : "second_example_gibbs_sampling.R",
    "properties" : {
        "tempName" : "Untitled1"
    },
    "relative_order" : 10,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_source"
}