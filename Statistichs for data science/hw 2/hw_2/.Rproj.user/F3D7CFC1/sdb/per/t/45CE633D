{
    "collab_server" : "",
    "contents" : "---\ntitle: 'Homework #1'\nauthor: |\n  | Stat4DS2+DS\n  | <https://elearning2.uniroma1.it/course/view.php?id=4951>\noutput:\n  pdf_document:\n    toc: true\nheader-includes:\n                - \\usepackage{graphicx}\n                - \\usepackage{bbm}\n\n---\n\n\n\n```{r setup, include=FALSE}\nknitr::opts_chunk$set(echo = TRUE, out.width='300px', dpi=200, fig.height = 4)\n```\n\n\nYour Last+First Name __MELE__UMBERTO_Jr_    Your Matricola _1388371_\n--------------\n\n\\newpage\n#Question 1\n\n## Part-a\n1a)  Illustrate the characteristics of the statistical model for dealing with the *Dugong*'s data. Lenghts ($Y_i$)  and Ages ($x_i$) of  27 Dugongs have been recorded and the\n  following (non linear)  regression model is considered:\n\\begin{eqnarray*}\nY_i &\\sim& N(\\mu_i, \\tau^2) \\\\\n\\mu_i=f(x_i)&=& \\alpha - \\beta \\gamma^{x_i}\\\\\n\\end{eqnarray*}\nModel parameters are\n$\\alpha \\in (1, \\infty)$,\n$\\beta \\in (1, \\infty)$,\n$\\gamma \\in (0,1)$,\n$\\tau^2 \\in (0,\\infty)$. \nLet us consider the following prior distributions:\n\\begin{eqnarray*}\n\\alpha &\\sim&  N(0,\\sigma^2_{\\alpha})\\\\\n\\beta  &\\sim&  N(0,\\sigma^2_{\\beta}) \\\\\n\\gamma &\\sim&  Unif(0,1)\\\\\n\\tau^2 &\\sim&  IG(a,b)) (Inverse Gamma)\n\\end{eqnarray*}\n\n```{r, include=FALSE}\nlibrary(igraph)\n```\n\n##### -Answer\n\nWe have a regression problem, where we assume that $\\mathbbm{E}(Y|X=x)= \\alpha - \\beta \\cdot \\gamma ^{x}$ and the noise $\\epsilon$ is distribuited like a normal with variance $\\tau^2$.\n\nSince we can absume that the parameters of the regression are indipendent we can rappresent this problem by a DAG , a graphical rapresentation. \n\nWhere variables $Y|X$ is the distribution that dipends on others parameters; while the other parameters of the problem are handling using some prior distribution that have the pourpose of give a kind of regularization to my loss function.\n\nImportant for our model is to write the join distribution in an appropriate way, so:\n\n$$J(Y,X,\\alpha,\\beta, \\gamma,\\tau^2)= P(Y|X,\\alpha,\\beta, \\gamma,\\tau^2) \\cdot P_{\\alpha}(\\alpha) \\cdot P_{\\beta}(\\beta) \\cdot P_{\\gamma}(\\gamma) \\cdot P_{\\tau^2}(\\tau^2) $$\n\n\n\n```{r, echo=FALSE}\nm = matrix(0,5,5)\ndimnames(m)=list(c(expression(Alpha),expression(Beta),expression(Gamma),\n                   'Y|X',expression(Tau)),\n                 c(expression(Alpha),expression(Beta),expression(Gamma),\n                   'Y|X',expression(Tau)))\nm[1,4]=1\nm[2, 4]=1\nm[3,4]=1\nm[5, 4]=1\nnet = graph.adjacency(m, mode = 'directed',diag = FALSE)\nV(net)$shape = 'circle'\nV(net)$color = 'white'\nV(net)$label.color = 'black'\nV(net)['Y|X']$shape = 'square'\nV(net)['Y|X']$color = 'grey'\nplot.igraph(net,vertex.label=c(expression(alpha),expression(beta),expression(gamma),\n                   'Y|X',expression(tau^2)),layout=layout.fruchterman.reingold,\n            vertex.size =setNames(c(25,25,25,40,25),c('Y|X',expression(alpha)\n                                                   ,expression(beta),expression(gamma),expression(tau^2))))\n\n```\n\\newpage\n\n## Part-b\n\n1b)  Derive the corresponding likelihood function\n\n##### -Answer\n\nWe have a set of observed data of 27 Dugong's Ages and Lengths, so from them we want to build a non-linear regression function such that the error is distribuited like a Normal with variance $\\tau^2$ and $\\mathbbm{E}(Y|X)= \\mu = \\alpha - \\beta\\cdot\\gamma^{x}$.\n\nSo computing the Maximum Likelihood Estimator we have almost the same effect, how we will see later, of the minimization of the least square error.\n\nSo, computing the likelihood:\n\n$$L(\\alpha,\\beta, \\gamma,\\tau^2 | (Y|X)) = \\prod_{i=1}^{27}P((Y=y_i|X=x_i) | \\alpha,\\beta, \\gamma,\\tau^2) = \\prod_{i=1}^{27} \\frac{1}{\\sqrt{2\\tau^2\\pi}}e^{-\\frac{(y_i-\\mu_i)^2}{2\\tau^2}} = \\prod_{i=1}^{27} \\frac{1}{\\sqrt{2\\pi\\tau^2}}e^{-\\frac{(y_i-\\alpha + \\beta\\gamma^{x_i})^2}{2\\tau^2}}$$\n\nand the negative log-likelihood function:\n\n$$- l(\\alpha,\\beta, \\gamma,\\tau^2 | (Y|X)) = 27\\cdot ln(\\sqrt{2\\pi\\tau^2}) + \\frac{1}{2\\tau^2}\\sum_{i=1}^{27}(y_i-\\alpha + \\beta\\gamma^{x_i})^2 \\propto \\sum_{i=1}^{27}(y_i-\\alpha + \\beta\\gamma^{x_i})^2  $$\n\nand that is the least square error problem with some inference on the noise.\n\n```{r, echo=FALSE}\ndat <- read.table('Dugong.txt', header = T)\nmu.func <- function(x, vector= vector){\n  alpha=vector[1]\n  beta = vector[2]\n  gamma = vector[3]\n  tau.square = vector[4]\n  mu = (alpha-beta* (gamma^(x)))\n  return(mu)\n}\n\nplot(dat$Age,dat$Length, col='black',xlab='Age',ylab='Length')\ncurve(mu.func(x, vector = c(3.2,1.5,0.95,0.15)), add = TRUE, col='blue')\n\n```\n\n\n\\newpage\n\n## Part-c\n\n1c)  Write down the expression of the joint prior distribution of the parameters at stake and illustrate some suitable choice for the hyperparameters.\n\n##### -Answer\n\nAnd we can suppose that our priors distributions are a type of regularization for our estimate paramaters. \n\nSo, considering the indipendence of the parameters the prior distribution can be written by a product of the distributions:\n\n$$\\pi( \\alpha,\\beta, \\gamma,\\tau^2)= \\pi_{\\alpha}(\\alpha) \\cdot \\pi_{\\beta}(\\beta)\\cdot \\pi_{\\gamma}(\\gamma)\\cdot \\pi_{\\tau^2}(\\tau^2)$$\n$$\\mathbbm{1}_{[1,\\infty]}(\\alpha)\\frac{1}{\\sigma_{\\alpha}\\sqrt{2\\pi}}e^{-\\frac{\\alpha^2}{2\\sigma_{\\alpha}}} \\cdot \\mathbbm{1}_{[1,\\infty]}(\\beta)\\frac{1}{\\sigma_{\\beta}\\sqrt{2\\pi}}e^{-\\frac{\\beta^2}{2\\sigma_{\\beta}}} \\cdot  \\mathbbm{1}_{[0,1]}(\\gamma) \\frac{1}{1-0} \\cdot \\mathbbm{1}_{[0,\\infty]}(\\tau^2)\\frac{b^{ a}}{\\Gamma(a)} \\tau^{2(-\\alpha -1)} e^{-\\frac{\\beta}{\\tau^2}} $$\n\n\nSo looking for each prior:\n\n- $\\alpha$ is the maximum value that the Lenght can reach when the age is infinity and giving to it a normal distribution with $\\mathbbm{E}(\\alpha)=0$ and $\\mathbbm{V}(\\alpha)=\\sigma_{\\alpha}$, it means for small values of the variance penalize more distance from 0, so to push this value close to 0.(I think should be interesting also play with the mean of this value such that it's possible to push up or down the regression function)\n\n- $\\beta$ is the coefficient of the pow so playing with its means have a bigger or smaller growth, but it's important also to define the intercept at the origin since when $x=0$:   $y= \\alpha - \\beta \\cdot 1$\nAnd we can think at it like we did with $alpha$. \n\n\n- $\\gamma$ define the speed of how the length converge to the maximum value $\\alpha$. So an appropriate prior is to give it equal probability for each value.\n\n- $\\tau^2$ rappresent's the variance of our error, so we could push it to be small or less small using the hyperparameters of an inverse-gamma distribution.\n\n\n\\newpage\n\n## Part-d\n\n1d)  Compute \\underline{numerically}  the maximum likelihood estimate for the vector of parameters of interest $(\\alpha , \\beta , \\gamma , \\tau)$ and compare it with the Maximum-a-Posterori estimate.\n\n##### -Answer\n\n\n```{r, include=FALSE}\nlibrary(MCMCpack)\n```\n\n\n```{r}\ndat <- read.table('Dugong.txt', header = T)\nLikelihood_3 <- function(vector){\n  Lik = 0\n  alpha=vector[1]\n  beta = vector[2]\n  gamma = vector[3]\n  tau.square = vector[4]\n  if(((alpha>1)*(beta>1)*(gamma>0 && gamma<1)\n      *(tau.square>0))==0){\n    Lik = 1e+10\n    return(Lik)\n  }\n  for( i in 1:nrow(dat)){\n    mu = (alpha-beta* (gamma^(dat$Age[i])))\n    if(mu <0){\n      Lik = 1e+10\n      return(Lik)\n    }\n    l = log(tau.square^(-0.5)) - ((dat$Length[i] - mu)^2 / (2*tau.square))\n    Lik = (sum(Lik, l))\n  }\n  return(-Lik)\n}\n\nMLE <- optim(c(1.1, 1.1, 0.2, 3),Likelihood_3)\n             \n\ncat('MLE alpha =',MLE$par[1],'\\n',\n    'MLE beta =',MLE$par[2],'\\n',\n    'MLE gamma =',MLE$par[3],'\\n',\n    'MLE tau.square =',MLE$par[4],'\\n',\n    '- log(Likelihood) optim =',Likelihood_3(MLE$par),'\\n',\n    'initial value - log(Likelihood) = ',Likelihood_3(c(1.1, 1.1, 0.1, 3)))\n\n```\n\n\\newpage\n\n\n```{r,eval=FALSE}\ni=1\nMLE <- optim(c(1.1, 1.1, 0.2, 3),Likelihood_3)\nres <- matrix(c(i,MLE$par,Likelihood_3(MLE$par)), ncol = 6)\n\nfor(a in 1:3){\n  for(b in 1:3){\n    for(g in 1:3){\n      for(t in 1:3){\n        i=i+1\n        cat(i,' ',Likelihood_3(c(1.1+(a-1)*1, 1.1+(b-1)*1, 0.2+(g-1)*0.1, 3+(t-1)*1)),'\\n')\n        cat(c(1.1+(a-1)*1, 1.1+(b-1)*1, 0.2+(g-1)*0.1, 3+(t-1)*1),'\\n')\n        MLE <- optim(c(1.1+(a-1)*1, 1.1+(b-1)*1, 0.2+(g-1)*0.1, 3+(t-1)*1),Likelihood_3)\n        res <- rbind(res,c(i, MLE$par, Likelihood_3(MLE$par)))\n      }\n    }\n  }\n}\n```\n\n```{r,echo=FALSE}\nload('risultati_mle.RData')\n```\n\n```{r}\nopt = unlist(res[res[,6]==min(res[,6]),][2:6])\ncat('MLE search alpha =',opt[1],'\\n',\n    'MLE search beta =',opt[2],'\\n',\n    'MLE search gamma =',opt[3],'\\n',\n    'MLE search tau.square =',opt[4],'\\n',\n    '- log(Likelihood) optim =',Likelihood_3(opt[1:4]),'\\n')\n```\n\n\n\n\n\n\\newpage\n\n\n\n\n```{r, warning=FALSE}\nPrior <- function(vector){\n  alpha=vector[1]\n  beta = vector[2]\n  gamma = vector[3]\n  tau.square = vector[4]\n  alpha.p <- dnorm(alpha,0,1e+3)*(alpha>=1)\n  beta.p <- dnorm(beta,0,1e+3)*(beta>=1)\n  gamma.p <- dunif(gamma,0,1)\n  tau.square.p <- dinvgamma(tau.square,1,1)*(tau.square>=0)\n  join <- log(alpha.p + 1e-10) + log(beta.p + 1e-10) + \n          log(gamma.p + 1e-10) + log(tau.square.p + 1e-10)\n  return(-join)\n}\n\nPosterior <- function(vector){Prior(vector)+Likelihood_3(vector)}\n\nMPO <- optim(c(1.5, 1.5, 0.02 , 3),Posterior)\nMPO$par\ncat('Maximum a Posterior alpha.hat =',MPO$par[1],'\\n',\n    'Maximum a Posterior beta.hat =',MPO$par[2],'\\n',\n    'Maximum a Posterior gamma =',MPO$par[3],'\\n',\n    'Maximum a Posterior tau.square =',MPO$par[4],'\\n',\n    ' -log(Posterior) local optim =',Posterior(MPO$par),'\\n',\n    ' -log(Posterior) initial value =',Posterior(c(1.1, 1.1, 0.2, 3)))\n```\n```{r, eval=FALSE}\ni=1\nMPO <- optim(c(1.1, 1.1, 0.2, 3),Posterior)\nres_mpo <- matrix(c(i,MPO$par,Posterior(MPO$par)), ncol = 6)\n\nfor(a in 1:3){\n  for(b in 1:3){\n    for(g in 1:3){\n      for(t in 1:3){\n        i=i+1\n        cat(i,' ',Posterior(c(1.1+(a-1)*1, 1.1+(b-1)*1, 0.2+(g-1)*0.1, 3+(t-1)*1)),'\\n')\n        cat(c(1.1+(a-1)*1, 1.1+(b-1)*1, 0.2+(g-1)*0.1, 3+(t-1)*1),'\\n')\n        MPO <- optim(c(1.1+(a-1)*1, 1.1+(b-1)*1, 0.2+(g-1)*0.1, 3+(t-1)*1),Posterior)\n        res_mpo <- rbind(res_mpo,c(i, MPO$par, Posterior(MPO$par)))\n      }\n    }\n  }\n}\n\n```\n\n\n```{r,echo=FALSE}\nload('risultati_mpo.RData')\n```\n\n```{r}\nopt_mpo = unlist(res_mpo[res_mpo[,6]==min(res_mpo[,6]),][2:6])\ncat('MPO search alpha =',opt[1],'\\n',\n    'MPO search beta =',opt[2],'\\n',\n    'MPO search gamma =',opt[3],'\\n',\n    'MPO search tau.square =',opt[4],'\\n',\n    '- log(Posterior) optim =',(opt[1:4]),'\\n')\n```\n\n\n\n\n```{r, out.width='400px', dpi=400, fig.height = 8}\nmu.func <- function(x, vector= vector){\n  alpha=vector[1]\n  beta = vector[2]\n  gamma = vector[3]\n  tau.square = vector[4]\n  mu = (alpha-beta* (gamma^(x)))\n  return(mu)\n}\n\nplot(dat$Age,dat$Length, col='black',xlab='Age',ylab='Length')\ncurve(mu.func(x, vector = opt[1:4]), add = TRUE, col='black')\ncurve(mu.func(x, vector = opt_mpo[1:4]), col = 'red', add= TRUE)\ncurve(mu.func(x, vector = MLE$par), add = TRUE, col='violet')\ncurve(mu.func(x, vector = MPO$par), col = 'green', add= TRUE)\nlegend(20,2, c(\"optim MLE\",\"optim MPO\",\"first MLE\",\"first MPO\"), lty = c(1,1,1,1),\n       lwd = c(2.5,2.5,2.5,2.5), col = c('black',\"red\",\"violet\",\"green\"))\n\n```\n\nAnd we can clearly see that both likelihood than posterior gives us the same result after some better optimization.\n\n\\newpage\n\n#Question 2\n\n\n2)  Consider the Acceptance-Rejection algorithm in the most general\n  form and denote with $\\theta=Y^A$ the random variable obtained with the algorithm\n\n\n## Part-a\n2a)  Determine the analytic expression of the acceptance probability\n\n#### - Answer\n\nWe want to derive the probabilty of $E = 1$, and we now that $E$ is a r.v. distribuited like a $Bernoulli$ with probabilty $p=\\frac{f(y)}{k g(y)}$.\n\nWhere $g_y(y)$ is our *instrumental density distribution*, while $f_x(X)$ is our *target density*, such that  $X\\subseteq Y$, while $k$ is choose to make the coinstrant $0 \\leq \\frac{f(y)}{k g(y)}\\leq 1$ so that it can be considered a probability.\n\n$$P(E = 1) = \\int J(E=1,Y=y)\\;dy = \\int P(E=1 | Y=y)\\cdot P(Y=y)\\;dy = \\int \\frac{f(y)}{k g(y)} \\cdot g(y)\\; dy$$\n$$P(E = 1)= \\frac{1}{k} \\int f(y) dy = \\frac{1}{k}$$\n\n## Part-b\n\n2b)  Prove that $\\theta$ has the desired target distribution\n\n##### -Answer\n\nSince $\\theta=Y^A$ we can write his probability: \n\n$$P(\\theta)= P(Y=y| E=1)= \\frac{P(E=1 | Y=y) \\cdot P(Y=y)}{P(E=1)} = \\frac{\\frac{f(y)}{k g(y)} \\cdot g(y)}{\\frac{1}{k}}= \\begin{cases}f(y) \\quad if\\quad y\\in X\\\\ 0\\quad otherwise \\end{cases}$$\n\nWhere the last equality is true since $f(y)=0$ if $y\\in Y-X$.\n\n\\newpage\n\n## Part-c\n\n2c)  Show how in Bayesian inference you could use simulations from\n  the prior (auxiliary density) to get a random  draw from the\n  posterior (target distribution)  without knowing the proportionality constant\n\n##### -Answer\n\nSuppose we have a function $f(x)$ from which we wish to simulate, but this function is not normalized, and his integral is difficult to compute analitically: $\\int f(x)\\; dx = K$, and we want to use as instrumental distribution $\\tilde q(y)$ with $x\\in X, \\quad y \\in Y, \\quad X\\subseteq Y$.\n\n(*Where $K = \\int \\frac{f(\\theta)}{\\tilde q(\\theta)}\\cdot \\tilde q(\\theta)\\; d\\theta$ could be computed using MC.*)\n\nWe can simulate from it, thinking in a Bayesian Inference setup.\n\nUsing: $$\\begin{cases}\n\\pi(\\theta | E) = \\frac{f(\\theta)}{K}\\\\\n\\pi(\\theta) = \\tilde q(\\theta)\\\\\nL_{E| \\theta}(\\theta) = Ber(\\frac{f(\\theta)}{M \\cdot \\tilde q(\\theta)})  \n\\end{cases}$$\n\nWhere $\\frac{f(\\theta)}{M \\cdot q(\\theta)}\\leq 1$  and $L() \\propto \\frac{\\pi(\\theta | E)}{\\pi(\\theta)} \\leq M\\cdot K$\n\nSo a problem could be to see if the Likelihood function is bounded or not... and this is not always true! \n\nThen if the Likelihood is bounded, we want to find an optimal value for $M$ since taking big values for it mean have a low probability of acceptance.\n\nSo dipending on the function $f(x)$ that we want to simulate the problem is to choose an appropriate prior distribution such that $M$ is the minimum value possible, and for this reason exist methods like \"Adaptive Rejection Sampling\" that using convexity properties of the target distribution, can reduce a lot the probabilty of acceptance.\n\n$$\\pi(\\theta | E) = \\frac{ L(E|\\theta)\\cdot \\pi(\\theta) }{\\int L(E|\\theta)\\cdot \\pi(\\theta)\\; d\\theta}$$\n\n\nSo the algorithm is:\n\n1) Simulate from the auxiliar distribution $\\pi(\\theta)$.\n2) Then we accept if $E=1 | Y=y \\sim Bernoulli(p = \\frac{1}{M} \\cdot \\frac{f(\\theta)}{\\tilde q(\\theta)})$\n\nWhere: $$P(E=1)=P(E=1|Y=y)\\cdot P(Y=y) \\int \\frac{1}{M} \\cdot \\frac{f(\\theta)}{\\tilde q(\\theta)} \\cdot \\pi(\\theta)\\; dy = \\frac{1}{M} \\cdot K $$\n\nExample of how use Monte Carlo to compute $K$:\n\nLet's use as auxiliar distribution a $Beta(1,1)$  to simuate r.v. from a function $f(x)=x^3(1-x)^6 \\quad whit \\quad x \\in [0,1]$\n\nwhere: $$\\int_0^1 x^3(1-x)^6 dx = B(4,7) = \\frac{\\Gamma(4)\\cdot \\Gamma(7)}{\\Gamma(4+7)}= \\frac{3! \\cdot 6!}{10!} = \\frac{6}{5040} = 0,00119$$\n\nIn this case is possible to compute the integral easy, but in other cases could be not so easy, but with montecarlo we approsimate it:\n\n\n```{r}\ndtarget<- function(x) x^3*(1-x)^6\na = 1\nb = 1\ndauxiliary <- function(x) dbeta(x,a,b)\nrauxiliary <- function(n) rbeta(n,a,b)\n\nsamp = runif(1e+5)\nk= mean((dtarget(samp)/dauxiliary(samp)))\nk\n\n```\n\n\n\n\\newpage\n\n## Part-d\n\n2d)  Illustrate analytically possible difficulties of this approach \nwith a simple conjugate model \n\n##### -Answer\n\nThe main problem of this approach is to find a good estimator of $M$ that bound the Likelihood function, but not too big because for  great values of $M$ we have a low probability of acceptance.\n\nThe natural choose for this number is of course the maximum value of $\\frac{f(y)}{\\tilde q(y)}$, but this number could be difficult to find analytically.\n\n$$sup \\quad \\frac{f(y)}{\\tilde q(y)}$$\nFor the sake of clarity let's play with some conjugate model; let's try to produce some random variable from a beta distribution with parameters $\\mathbbm{B}(4,7)$, sampling from a beta $\\mathbbm{B}(1,1)$ that is a uniform distribution.\n\nSo, we have to find:\n\n$$sup \\quad \\frac{f(y)}{\\tilde q(y)}=  sup \\quad \\frac{\\frac{x^{4-1}(1-x)^{7-1}}{\\mathbbm{B}(4,7)}}{1}$$\nand: $$ \\frac{d}{dy}\\frac{f(y)}{\\tilde q(y)} = 3x^2(1-x)^6 - 6x^3(1-x)^5 = 0$$\n\nthat has the maximum in $\\frac{3}{9}$, but not evry time it's possible to find this maximum if the function it's not bounded.\n\n\nAnother issues is that the probabilty of Acceptance decrease if we have too many observations, because K go down faster than how M decrease.\n\nRemember that $P(E=1)=\\frac{K}{M}$\n\n\\newpage\n\n## Part-e\n\n2e)  Verify your conclusions implementing the Acceptance-Rejection\n  approach with  your conjugate model \n(verify empirically that  $\\theta$ has the desired target distribution $\\pi(\\theta|x_1,..,x_n)$ )\n\n##### -Answer    \n\nLet's use as auxiliar distribution a $Beta(5,8)$  to simuate r.v. from a function $f(x)=x^3(1-x)^6 \\quad whit \\quad x \\in [0,1]$\n\n\n```{r}\ndtarget<- function(x) x^3*(1-x)^6\na = 2\nb = 2\ndauxiliary <- function(x) dbeta(x,a,b)\nrauxiliary <- function(n) rbeta(n,a,b)\n\nsamp = runif(1e+5)\nM = max((dtarget(samp)/dauxiliary(samp)))\nk= mean((dtarget(samp)/dauxiliary(samp)))\nM = M*(1.1)\n\nAR=function(dtarget,dauxiliary,rauxiliary,M){\n  count=0\n  E=0\n  while(E==0){\n    candidate = rauxiliary(1)\n    acc_prob= (dtarget(candidate)/(M*dauxiliary(candidate)))\n    E = sample(c(1,0),prob=c(acc_prob, 1-acc_prob),size=1)\n    count=count+1\n  }\n  return(list(draw=candidate,computational_effort=count))\n}\n\ncurve(dauxiliary(x)*M, col ='blue')\ncurve(dtarget, col= 'red',add=T)\n```\n\n```{r}\nmcsize=1000\ndraw_vec=rep(NA,mcsize)\neffort_vec=rep(NA,mcsize)\n\nfor(i in 1:mcsize){\n  \n  DD=AR(dtarget,dauxiliary,rauxiliary,M)\n  draw_vec[i] = DD$draw\n  effort_vec[i] = DD$computational_effort\n  \n}\n\nhist(draw_vec,freq=FALSE)\ncurve(dtarget(x),add=TRUE)\n\nplot(prop.table(table(effort_vec)),ylim=c(0,1),pch=16,col=\"red\")\n\n```\n\n\n\n\\newpage\n\n#Question 3\n\n3)  Simulate from a standard Normal distribution using \npseudo-random deviates from a standard Cauchy \nand the A-R algorithm. Write the expression the corresponding  acceptance probabilityof and evaluate it numerically by MC approximation.\n\n\n##### -Answer\n\nTo use pseudo-random deviates to sample from a standad Cauchy, we should use the fact that the Cumulative Density Function is invertible compute the inverse of the Cumulative Density Function to make \n\n\n\n\n$P(E=1)= \\int P(e=1 | Y=y) \\cdot P(Y=y)\\;dy = \\int \\frac{f(y)}{M\\cdot g(y)} g(y)\\; dy = \\frac{1}{M} \\int f(y)\\; dy$\n\nwhere M is the minum M such that $\\frac{f(y)}{M\\cdot g(y)}\\leq 1$  $\\forall y \\in Y$.\n\nSo numerically we should find:\n\n$$sup \\; \\frac{f(y)}{g(y)} = sup \\; \\frac{\\frac{1}{\\sqrt{2 \\pi}}\\cdot e^{-\\frac{x^2}{2}}}{\\frac{1}{\\pi\\cdot(1 + x^2)}} = sup\\; \\frac{\\pi\\cdot(1+x^2)}{\\sqrt{2\\pi}}\\cdot e^{-\\frac{x^2}{2}}$$\n\nso:\n\n$$ \\frac{d}{dy}\\cdot \\frac{f(y)}{g(y)} = 2x \\frac{\\pi}{\\sqrt{2\\pi}}\\cdot e^{-\\frac{x^2}{2}} \\; - x \\frac{\\pi\\cdot(1+x^2)}{\\sqrt{2\\pi}}\\cdot e^{-\\frac{x^2}{2}} = e^{-\\frac{x^2}{2}}\\cdot \\frac{\\pi}{\\sqrt{2 \\pi}}\\cdot(2x - x \\cdot (1 + x^2))$$\n \nthat is equal to zero and mazimum in $x = \\pm 1$, and so $M= \\sqrt{\\frac{2\\pi}{e}}$ and of course:\n\n$$P(E=1) = \\frac{1}{M} = \\sqrt{ \\frac{e}{2\\pi}} = 0.66$$\n\nSo numerically let's see:\n\n\n```{r}\ndtarget<- function(x) dnorm(x)\n\ndauxiliary <- function(x) dcauchy(x)\nrauxiliary <- function(n) rcauchy(n)\n\nsamp = rauxiliary(1e+5)\nM = max((dtarget(samp)/dauxiliary(samp)))\n\nM = M*(1.0001)\nAR=function(dtarget,dauxiliary,rauxiliary,M){\n  count=0\n  E=0\n  while(E==0){\n    candidate = rauxiliary(1)\n    acc_prob= (dtarget(candidate)/(M*dauxiliary(candidate)))\n    E = sample(c(1,0),prob=c(acc_prob, 1-acc_prob),size=1)\n    count=count+1\n  }\n  return(list(draw=candidate,computational_effort=count))\n}\n\ncurve(dauxiliary(x)*M, col ='blue', from = -10, to = 10)\ncurve(dtarget, col= 'red',add=T)\n\n```\n\nwhere M is:\n\n```{r, echo=FALSE}\n\nc(\"M \" = M , \"    P(E=1)\"= 1/M)\n\n```\n\n\n\n```{r}\nmcsize=1000\ndraw_vec=rep(NA,mcsize)\neffort_vec=rep(NA,mcsize)\n\nfor(i in 1:mcsize){\n  \n  DD=AR(dtarget,dauxiliary,rauxiliary,M)\n  draw_vec[i] = DD$draw\n  effort_vec[i] = DD$computational_effort\n  \n}\n\nhist(draw_vec,freq=FALSE)\ncurve(dtarget(x),add=TRUE)\n\n```\n\n\\newpage\n\n# Question 4\n\n4)  Let us consider a Markov chain \n$(X_t)_{t \\geq 0}$\ndefined on the state space ${\\cal S}=\\{1,2,3\\}$\nwith the following transition \n\n\n\n\\begin{center} \n\\includegraphics[width=6cm]{frog.png} \n\\end{center}\n\n## Part-a and -b\n\n4a)  Starting at time $t=0$ in the state  $X_0=1$\nsimulate the Markov chain with distribution assigned as above\nfor $t=1000$ consecutive times\n\nb)  compute the empirical relative frequency of the three states in\n  your simulation\n\n\n##### -Answer\n\n```{r}\nstates <- c(1,2,3)\ntransition_matrix <- matrix(data= c(0,1/2,1/2,\n                                    5/8,1/8,1/4,\n                                    2/3,1/3,0), byrow = TRUE ,nrow=3)\n\n\n\n\nt0 <- 1\n\nnsample<-1000\nchain<-rep(NA,nsample+1) # vector that will hold\n# all the simulated values\nchain[1]<-t0             # starting value x1 assigned to chain[1]\nfor(t in 1:nsample){\n  chain[t+1]<-sample(states,size=1,prob=transition_matrix[chain[t],])\n}\n\nprop.table(table(chain))\n\n```\n\n\n\n\n\n\\newpage\n  \n  \n  \n## Part-c\n\n4c)  repeat the simulation for 500 times and record only \nthe final state at time $t=1000$ for each of the \n    500 simulated chains. Compute the relative\n  frequency of the 500 final states.\nWhat distribution are you approximating in this way?  \nTry to formalize the difference between this point and the previous\npoint. \n\n\n##### -Answer\n\n```{r}\nt0 <- 1\n\nntimes <- 500\nnsample<-1000\nt1000 <-rep(NA,ntimes)\n\nfor(i in 1:ntimes){\n  chain<-rep(NA,nsample+1) # vector that will hold\n  chain[1]<-t0             # starting value x1 assigned to chain[1]\n  for(t in 1:nsample){\n    chain[t+1]<-sample(states,size=1,prob=transition_matrix[chain[t],])\n  }\n  t1000[i]<- chain[nsample+1]\n}\n\nprop.table(table(t1000))\n\n```\n\nIn this last case we're trying to approximate the distribution of the invariant distribution using only the kernel matrix $K^{1000}(x_0 =1 , \\Omega)$.\n\nWhile in the preview simulation we were approximating the same invariant distribution, but using a random walk given by the simulation that start from $x_0$, that is given by the chain originated by the transition kernel s.t. :\n\n$$K(x_{t-1}, x_t) \\quad \\forall \\; t =1,2 ... ,1000$$\nThis first case have the issues of take into account the initial point and the correlation on the preview state in time, so the variance of his estimate his bigger in respect of the second estimate, but instead the first one consume less computation than the second.\n\n\n\\newpage\n\n\n## Part-d\n\n4d)  compute the theoretical stationary distribution $\\pi$ and explain how\n  you have obtained it\n\n##### -Answer\n\nIn theory the stationary distribution it can be computed looking for the eigen vector related to the eigen value equal to 1.\nBut in R the function that gives us this this vector doesn't work so well:\n\n\n```{r}\neigen(transition_matrix)\n\neigen(transition_matrix)$vectors[1,]%*%transition_matrix\neigen(transition_matrix)$vectors[1,]\n\n```\n\nSo it's possible to computed it using an high power of the transition matrix:\n\n(*I used markovchain library*)\n\n\n```{r, include=FALSE}\nlibrary(markovchain)\nstates <- c('1','2','3')\nByRow <- TRUE\ntransition_matrix <- matrix(data= c(0,1/2,1/2,\n                                    5/8,1/8,1/4,\n                                    2/3,1/3,0), byrow = ByRow,nrow=3)\nMc_states <- new('markovchain', states = states, byrow = ByRow, transitionMatrix = transition_matrix, name='Position')\n\n```\n\n```{r}\n(Mc_states^1000)[1,]\n```\n\n\n\n\n## Part-e\n\n4e)  is it well approximated by the simulated empirical relative\n  frequencies computed in (b) and (c)?\n\n##### -Answer\n\nThe simulation computed in b) it's not good such the one computed in c), because like I said before the variance of the estimate in b) is grater than the one in c).\n\nBut despite everything are not bad both.\n\n  \n\n\\newpage  \n  \n## Part-f\n  \n4f)  what happens if we start at $t=0$ from state \n$X_0=2$ instead of  $X_0=1$?\n\n##### -Answer\n\n\n\n```{r}\nstates <- c(1,2,3)\ntransition_matrix <- matrix(data= c(0,1/2,1/2,\n                                    5/8,1/8,1/4,\n                                    2/3,1/3,0), byrow = TRUE ,nrow=3)\n\n\n\n\nt0 <- 2\n\nnsample<-1000\nchain1<-rep(NA,nsample+1) # vector that will hold\n# all the simulated values\nchain1[1]<-t0             # starting value x1 assigned to chain[1]\nfor(t in 1:nsample){\n  chain1[t+1]<-sample(states,size=1,prob=transition_matrix[chain1[t],])\n}\n\nprop.table(table(chain))\n\n\nntimes <- 500\nnsample<-1000\nt1000 <-rep(NA,ntimes)\n\nfor(i in 1:ntimes){\n  chain2<-rep(NA,nsample+1) # vector that will hold\n  chain2[1]<-t0             # starting value x1 assigned to chain[1]\n  for(t in 1:nsample){\n    chain2[t+1]<-sample(states,size=1,prob=transition_matrix[chain2[t],])\n  }\n  t1000[i]<- chain2[nsample+1]\n}\n\nprop.table(table(t1000))\n\n```\n\nHow we said before this approxiamtion dipends on the initial state, so our estimate stationary distribution change from the previouse ones.\n\n\n\n\\newpage\n\n\n# Question-5\n\n5)  Consider again the Bayesian model for Dugong's data:\n\n## Part-a and -b\n\n5a)  Derive the functional form  (up to proportionality constants) of all *full-conditionals*\n    b)  Which distribution can you recognize within standard parametric\n  families\nso that direct simulation from full conditional can be easily implemented ?\n\n##### -Answer\n\nTo compute the full-conditionals PDF let's compute before the join posterior distributions:\n\n$$\\pi(\\alpha, \\beta, \\gamma, \\tau^2 | (Y|X)) \\propto (\\tau^2)^{-\\frac{n}{2}}exp\\Big \\{ - \\frac{ n\\alpha^2 + \\sum_i y_i^2 + \\sum_i \\beta^2\\gamma^{2 x_i} - 2\\alpha(\\sum_i (y_i + \\beta\\gamma^{x_i})) + 2\\sum_i(y_i\\beta\\gamma^{x_i})}{2\\tau^2}  \\Big\\}\\cdot $$\n$$\\cdot exp \\Big\\{-\\frac{\\alpha^2}{2\\sigma_{\\alpha}^2} -\\frac{\\beta^2}{2\\sigma_{\\beta}^2} -\\frac{b}{\\tau^2}  \\Big\\}\\cdot \\tau^{2(-a-1)} \\qquad \\forall \\alpha \\in (1;\\infty), \\beta \\in (1;\\infty), \\gamma \\in (0,1), \\tau^2 \\in (0;\\infty)$$\n\nSo to try to implement the Gibbs sampling, we have to derive analitically the distribution probability functions of each\nfull-conditionals posteriors.\n\nSo for $\\alpha$:\n\n$$\\pi(\\alpha| \\beta, \\gamma, \\tau^2, Y|X) \\propto exp\\Big\\{ -\\frac{n}{2\\tau^2}\\cdot \\alpha^2 + \\frac{\\sum_i (y_i + \\beta \\cdot \\gamma^{x_i})}{\\tau^2}\\alpha - \\frac{1}{2 \\sigma_{\\alpha}^2} \\alpha^2 \\Big\\}\\mathbbm{1}_{(0,\\infty)}(\\alpha)$$\n$$\\alpha' \\sim N_{\\alpha'}\\left(a= \\frac{\\tau^2 + n\\sigma_{\\alpha}^2}{\\tau^2\\cdot \\sigma_{\\alpha}^2}; b= \\frac{\\sum_i (y_i + \\beta \\cdot \\gamma^{x_i})}{\\tau^2} \\right) \\qquad \\forall \\alpha' \\in (1,\\infty)$$\n\nSo for $\\beta$:\n\n\n\n$$\\pi(\\beta| \\alpha, \\gamma, \\tau^2, Y|X) \\propto exp\\Big\\{ -\\frac{\\sum_i\\gamma^{2x_i}}{2\\tau^2}\\cdot \\beta^2 + \\frac{(\\sum_i (\\alpha - y_i)\\gamma^{x_i})}{\\tau^2}\\beta - \\frac{1}{2 \\sigma_{\\beta}^2} \\beta^2 \\Big\\}\\mathbbm{1}_{(0,\\infty)}$$\n$$ \\beta' \\sim N_{\\beta'}\\left(a= \\frac{\\sigma_{\\beta}^2\\sum_i \\gamma^{2x_i} + \\tau^2}{\\tau^2\\cdot \\sigma_{\\beta}^2}; b= \\frac{(\\sum_i (\\alpha - y_i)\\gamma^{x_i})}{\\tau^2} \\right) \\qquad \\forall \\beta' \\in (1;\\infty)$$\n\nfor $\\gamma$:\n\n$$\\pi(\\gamma | \\alpha, \\beta, \\tau^2, Y|X) \\propto exp\\Big\\{ -\\frac{\\sum_i [\\beta\\gamma^{x_i}(\\beta\\gamma^{x_i}-2\\alpha +2y_i)]}{2\\tau^2}\\Big\\} \\qquad \\gamma\\in(0;1)$$\n\\newpage\n\nfor $\\tau^2$:\n\n$$\\pi(\\tau^2| \\gamma, \\alpha, \\beta, Y|X) \\propto (\\tau^2)^{-\\frac{n}{2}}\\cdot exp \\Big\\{-\\frac{\\sum_i(y_i-\\alpha+\\beta\\gamma^{x_i})^2}{2\\tau^2}\\Big\\}\\cdot exp\\Big\\{-\\frac{b}{\\tau^2} \\Big\\}\\cdot\\tau^{2(-a-1)}$$\n\n$$\\propto exp\\Big\\{ -\\frac{\\frac{1}{2}\\sum_i(y_i-\\alpha+\\beta\\gamma^{x_i})^2 + b}{\\tau^2} \\Big\\}\\cdot \\tau^{2(-a-\\frac{n}{2}-1)} $$ \n\n$$\\tau_{,}^{2} \\sim IG(\\alpha=a+\\frac{n}{2} \\; ; \\;\\beta = \\frac{1}{2}\\sum_i(y_i-\\alpha+\\beta\\gamma^{x_i})^2 + b) \\qquad \\forall \\tau^2 \\in (0;\\infty)$$\n\n\n\nSo we have two normal distributions, one inverse gamma distribution and something that it's a distribution of probability because it's bounded and strictly positive, but it's not a distribution from standards families.\n\n\\newpage\n\n## Part-c\n\n5c)  Using a suitable Metropolis-within-Gibbs algorithm simulate a Markov chain \n($T=10000$) to approximate the posterior distribution for the above model\n\n\n##### -Answer\n\n```{r, eval=FALSE}\n\nlibrary(truncnorm)\n\nsigma_alpha <- 1000\n\nfull.conditional.alpha <- function(start){\n  al = (1/(sigma_alpha^2)) + (27/start['tau.square'])\n  be = ((start['beta']*(sum(start['gamma']^dat$Age)))   +  \n         sum(dat$Length))/ start['tau.square']\n  sim_alpha <- 0\n  while(sim_alpha<=1){\n    sim_alpha <- rtruncnorm(1,mean = be/al , sd = sqrt(1/al), a =1)\n  }\n  return(sim_alpha)\n}\n\nsigma_beta <- 1000\n\nfull.conditional.beta <- function(start){\n  al = ((sum(start['gamma']^(2*dat$Age))/start['tau.square']) + (1/(sigma_beta^2)))\n  be = (start['alpha']*sum(start['gamma']^dat$Age)  -  \n         sum(dat$Length * (start['gamma']^dat$Age))) / start['tau.square']\n  sim_beta <- 0\n  while(sim_beta<=1){\n    sim_beta <- rtruncnorm(1,mean = be/al , sd = sqrt(1/al), a=1)\n  }\n  return(sim_beta)\n}\n\n\ngamma.function.ugly.distribution <- function(gam, start){\n  alp.n = start['alpha']\n  bet.n = start['beta']\n  tau.2.n = start['tau.square']\n  mu.n = alp.n - bet.n*(gam^dat$Age)\n  ret <- exp(- (1/tau.2.n)*0.5*sum((dat$Length - mu.n)^2))\n  return(ret)\n}\n\n\nfull.conditional.gamma.1 <- function(start){\n  gamma.new <- runif(1)\n  check_MH <- runif(1)\n  gamm <- start['gamma']\n  prob <- min(((gamma.function.ugly.distribution(gamma.new, start))/(gamma.function.ugly.distribution(gamm,start))),1)\n  check_MH <- rbinom(1,1,prob = prob)\n  if(check_MH){\n    gamm <- gamma.new\n  }\n  return(gamm)\n}\n\nal <- 3\nbe <- 0.5\n\nfull.conditional.tau.square <- function(start){\n  alpha.n <- start['alpha']\n  beta.n <- start['beta']\n  gamma.n <- start['gamma']\n  a = al + 27/2\n  b = ((1/2)*sum((dat$Length - alpha.n + beta.n*(gamma.n^dat$Age))^2)) + be \n  tau.square.sim <- rinvgamma(1,a,b)\n  return(tau.square.sim)\n}\n\n\nstart<- c('alpha'=1.1, 'beta'=1.01, 'gamma'=0.87,'tau.square'=0.007)\n\nn=10000\nsimulation1 <- matrix(nrow = n, ncol= 4)\nfor(i in 1:n){\n  start['alpha']= full.conditional.alpha(start)\n  start['beta']= full.conditional.beta(start)\n  start['gamma'] = full.conditional.gamma.1(start)\n  start['tau.square'] = full.conditional.tau.square(start)\n  simulation1[i,]=start\n}\n\nm <- mcmc(simulation1)\nAcceptanceRate(m)\n\n\n```\n\n\n```{r, include=FALSE}\nlibrary(LaplacesDemon)\nlibrary(mcmc)\nload(file = 'simulation1.RData')\n\n```\n```{r}\nAcceptanceRate(simulation1)\n```\n\n\n\n\\newpage\n\n### Part d, e & f  \n\n5d)  Show the 4 univariate trace-plots of the simulations of each parameter\n\ne)  Evaluate graphically the behaviour of the empirical averages \n$\\hat{I}_t$  with growing $t=1,...,T$\n\nf)  Provide estimates for each parameter together with the\n  approximation error and explain how you have evaluated such error\n\n\n##### -Answer\n\n\nfor $\\alpha$, the simulation gives me this result: \n\n```{r}\nsummary(simulation1[,1])\n\npar(mfrow=c(1,2))\n\nplot(simulation1[,1], type = 'l', ylab='alpha random walk')\nhist(simulation1[,1], breaks = 150, main = 'hist of alpha', xlab = 'values of simulation' )\n\n```\n\n\nAnd the graphically behaviour of the empirical averages, is:\n\n\n\n```{r}\npar(mfrow=c(1,2))\n\nacf(simulation1[,1], main ='alpha acf')\nplot(cumsum(simulation1[,1])/seq_along(simulation1[,1]), type='l',\n     xlab ='index' ,ylab='estimate chain for alpha')\n```\n\n\n\nwhere:\n\n\n$$ \\hat I = \\frac{1}{t} \\sum^t \\theta_t \\quad \\forall\\; \\theta_t \\sim \\pi(\\theta)$$\n\n$$\\mathbbm{E}(\\theta) = \\int \\theta \\pi(\\theta) \\; d\\theta \\;\\approx  \\frac{1}{t} \\sum^t \\theta_t \\quad \\forall\\; \\theta_t \\sim \\pi(\\theta)$$\n\n\nand the variance of the estimation can be computed using 'Bacth Means':\n\n$$\\mathbbm{V}(\\hat I) = B \\frac{1}{\\frac{t}{B} - 1} \\sum_{b=1}^{B}(\\hat I_{(b)} - \\hat I_t)^2$$\n\n\nAnd i used 'mcmcse' packet to compute the standard error (computed using batch means with size equal to $\\sqrt{10000}=100$):\n\n```{r, include=FALSE}\nlibrary(knitr)\n```\n\n\n\n```{r}\nc(\"mean estimator for alpha\" = mean(simulation1[,1]),\"standard error\"= mcmcse::mcse(simulation1[,1])$se)\n```\n\n\n\n\n\n\n\n\\newpage\n\nfor $\\beta$, I have this results:\n\n\n```{r, echo=FALSE}\nsummary(simulation1[,2])\n\npar(mfrow=c(1,2))\n\nplot(simulation1[,2], type = 'l', ylab='beta random walk')\nhist(simulation1[,2], breaks = 150, main = 'hist of beta', xlab = 'values of simulation')\n\n```\n\n```{r, echo=FALSE}\npar(mfrow=c(1,2))\n\nacf(simulation1[,2], main ='beta acf')\nplot(cumsum(simulation1[,2])/seq_along(simulation1[,2]), type='l',\n     xlab ='index' ,ylab='estimate chain for beta')\n\nc(\"mean estimator for beta\" = mean(simulation1[,2])  ,\"standard error\"= mcmcse::mcse(simulation1[,2])$se)\n```\n\n\n\n\\newpage\n\nfor $\\tau^2$, I have this results: \n\n\n\n```{r, echo=FALSE}\nsummary(simulation1[,4])\n\npar(mfrow=c(1,2))\n\nplot(simulation1[,4], type = 'l', ylab='tau.square random walk')\nhist(simulation1[,4], breaks = 150, main = 'hist of tau.square',\n     xlim = c(0,20), xlab ='values of simulation')\n\n```\n\n```{r, echo=FALSE}\npar(mfrow=c(1,2))\n\nacf(simulation1[,4], main ='tau.square acf')\nplot(cumsum(simulation1[,4])/seq_along(simulation1[,4]), type='l',\n     xlab ='index' ,ylab='estimate chain for tau.square')\n\nc(\"mean estimator for tau.square\" = mean(simulation1[,4])  ,\"standard error\"= mcmcse::mcse(simulation1[,4])$se)\n```\n\n\n\\newpage\n\nfor $\\gamma$, I have this results: \n\n```{r, echo=FALSE}\nsummary(simulation1[,3])\n\npar(mfrow=c(1,2))\n\nplot(simulation1[,3], type = 'l', ylab='gamma random walk')\nhist(simulation1[,3], breaks = 150, main = 'hist of gamma',\n     xlab ='values of simulation', probability = T)\n```\n\n```{r, echo=FALSE}\npar(mfrow=c(1,2))\n\nacf(simulation1[,3], main =\"gamma's acf\")\nplot(cumsum(simulation1[,3])/seq_along(simulation1[,3]), type='l',\n     xlab ='index' ,ylab='estimate chain for gamma')\n\nc(\"mean estimator for gamma\" = mean(simulation1[,3])  ,\"standard error\"= mcmcse::mcse(simulation1[,3])$se)\n```\n\n\n\\newpage\n\n## Part -g\n\n5g)  Which parameter has the largest posterior uncertainty? How did\n  you measure it?\n\n##### -Answer\n\nfor the posterior uncertainty, i used the formula of  the 'relative error':\n\n$$ \\frac{\\sqrt{\\mathbbm{V}(\\theta)}}{\\mathbbm{E}(\\theta)}$$\n```{r}\nc(\"relative error for alpha\"=mcmcse::mcse(simulation1[,1])$se/mean(simulation1[,1]) ,\n  \"relative error for beta\"=mcmcse::mcse(simulation1[,2])$se/mean(simulation1[,2])\n  ,\"relative error for gamma\"=mcmcse::mcse(simulation1[,3])$se/mean(simulation1[,3]),\n  \"relative error for tau.square\"=mcmcse::mcse(simulation1[,4])$se/mean(simulation1[,4]))\n\n```\n\n\nSo the parameter with the largest uncertaintly is gamma.\n\n\n## Part -h\n\n5h)  Which couple of parameters has the largest correlation (in\n  absolute value)?\n  \n##### -Answer  \n\n  \n```{r,include=FALSE}\nlibrary(corrplot)\n```\n```{r}\ncolnames(simulation1) <- c(\"alpha\",\"beta\", 'gamma','tau.square')\ncorrplot(cor(simulation1),method = \"number\")\n```\n\n\n\\newpage\n\n## Part -i and -j\n\n5i)  Use the Markov chain to approximate the posterior predictive \ndistribution of the length\n  of a dugong with age of 20 years.\n  \nj)  Provide the prediction of another  dugong with age 30 \n\n```{r}\nlikel <- function(vector,xi){\n  alpha.hat=vector[1]\n  beta.hat =vector[2] \n  gamma.hat =vector[3]\n  tau.square.hat = vector[4]\n  mu.hat = (alpha.hat - beta.hat*(gamma.hat^(xi)))\n  resul  <- rnorm(n = 1, mean = mu.hat, sd = sqrt(tau.square.hat))\n  return(resul)\n\n  }\nposterior_predictive_20 <- matrix(nrow = dim(simulation1)[1])\nfor(ind in 1:(dim(simulation1)[1])){\n  posterior_predictive_20[ind] <- likel(simulation1[ind,],20) \n}\n\nhist(posterior_predictive_20)\n\n```\n\n\n```{r}\nposterior_predictive_30 <- matrix(nrow = dim(simulation1)[1])\nfor(ind in 1:(dim(simulation1)[1])){\n  posterior_predictive_30[ind] <- likel(simulation1[ind,],30) \n}\n\nhist(posterior_predictive_30)\n```\n\n## Part -k\n\nk)  Which prediction is less precise?\n\n##### -Answer\n\nUsing the 'relative error':\n\n```{r, echo=FALSE}\nc(\"relative error for prediction of a dugong with 20 years\"=mcmcse::mcse(posterior_predictive_20)$se/mean(posterior_predictive_20) ,\n  \"relative error for prediction of a dugong with 30 years\"=mcmcse::mcse(posterior_predictive_30)$se/mean(posterior_predictive_30))\n```\n\n\n\\vspace*{6cm}\n\n\n* * *\n<div class=\"footer\"> &copy; 2016-2017 - Stat4DS2+CS - Luca Tardella </div>\n\n```{r, warning=FALSE, error=FALSE, message=FALSE, echo=FALSE}\ncat(paste0(\"This homework will be graded and it will be part of your final evaluation \\n\\n \"))\ncat(paste(\"Last update by LT:\",date()))\n```\n\n\n",
    "created" : 1494175237523.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "3913203412",
    "id" : "45CE633D",
    "lastKnownWriteTime" : 1496008284,
    "last_content_update" : 1496008284846,
    "path" : "C:/Users/Umbertojunior/Desktop/data science/Second Semestr/SDS 2/hw 2/hw_2/1388371_mele.Rmd",
    "project_path" : "1388371_mele.Rmd",
    "properties" : {
        "chunk_output_type" : "inline",
        "last_setup_crc32" : "13D8ACBCef6b1899",
        "tempName" : "Untitled1"
    },
    "relative_order" : 1,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_markdown"
}